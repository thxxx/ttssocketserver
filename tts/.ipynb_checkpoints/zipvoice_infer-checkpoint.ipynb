{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ca0b57-6bfd-44ed-af54-aeb782edbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import safetensors.torch\n",
    "import torch\n",
    "import torchaudio\n",
    "from huggingface_hub import hf_hub_download\n",
    "from lhotse.utils import fix_random_seed\n",
    "from vocos import Vocos\n",
    "\n",
    "from zipvoice.models.zipvoice import ZipVoice\n",
    "from zipvoice.models.zipvoice_distill import ZipVoiceDistill\n",
    "from zipvoice.tokenizer.tokenizer import (\n",
    "    EmiliaTokenizer,\n",
    "    EspeakTokenizer,\n",
    "    LibriTTSTokenizer,\n",
    "    SimpleTokenizer,\n",
    ")\n",
    "from zipvoice.utils.checkpoint import load_checkpoint\n",
    "from zipvoice.utils.common import AttributeDict\n",
    "from zipvoice.utils.feature import VocosFbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cf0523-b4f9-4235-8ec7-0c0854babc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_REPO = \"k2-fsa/ZipVoice\"\n",
    "MODEL_DIR = {\n",
    "    \"zipvoice\": \"zipvoice\",\n",
    "    \"zipvoice_distill\": \"zipvoice_distill\",\n",
    "}\n",
    "\n",
    "def get_vocoder(vocos_local_path: Optional[str] = None):\n",
    "    if vocos_local_path:\n",
    "        vocoder = Vocos.from_hparams(f\"{vocos_local_path}/config.yaml\")\n",
    "        state_dict = torch.load(\n",
    "            f\"{vocos_local_path}/pytorch_model.bin\",\n",
    "            weights_only=True,\n",
    "            map_location=\"cpu\",\n",
    "        )\n",
    "        vocoder.load_state_dict(state_dict)\n",
    "    else:\n",
    "        vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\")\n",
    "    return vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb2af3d-1ff0-428e-8dc8-d6533a15bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(\n",
    "    save_path: str,\n",
    "    prompt_text: str,\n",
    "    prompt_wav: str,\n",
    "    text: str,\n",
    "    model: torch.nn.Module,\n",
    "    vocoder: torch.nn.Module,\n",
    "    tokenizer: EmiliaTokenizer,\n",
    "    feature_extractor: VocosFbank,\n",
    "    device: torch.device,\n",
    "    num_step: int = 16,\n",
    "    guidance_scale: float = 1.0,\n",
    "    speed: float = 1.0,\n",
    "    t_shift: float = 0.5,\n",
    "    target_rms: float = 0.1,\n",
    "    feat_scale: float = 0.1,\n",
    "    sampling_rate: int = 24000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate waveform of a text based on a given prompt\n",
    "        waveform and its transcription.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): Path to save the generated wav.\n",
    "        prompt_text (str): Transcription of the prompt wav.\n",
    "        prompt_wav (str): Path to the prompt wav file.\n",
    "        text (str): Text to be synthesized into a waveform.\n",
    "        model (torch.nn.Module): The model used for generation.\n",
    "        vocoder (torch.nn.Module): The vocoder used to convert features to waveforms.\n",
    "        tokenizer (EmiliaTokenizer): The tokenizer used to convert text to tokens.\n",
    "        feature_extractor (VocosFbank): The feature extractor used to\n",
    "            extract acoustic features.\n",
    "        device (torch.device): The device on which computations are performed.\n",
    "        num_step (int, optional): Number of steps for decoding. Defaults to 16.\n",
    "        guidance_scale (float, optional): Scale for classifier-free guidance.\n",
    "            Defaults to 1.0.\n",
    "        speed (float, optional): Speed control. Defaults to 1.0.\n",
    "        t_shift (float, optional): Time shift. Defaults to 0.5.\n",
    "        target_rms (float, optional): Target RMS for waveform normalization.\n",
    "            Defaults to 0.1.\n",
    "        feat_scale (float, optional): Scale for features.\n",
    "            Defaults to 0.1.\n",
    "        sampling_rate (int, optional): Sampling rate for the waveform.\n",
    "            Defaults to 24000.\n",
    "    Returns:\n",
    "        metrics (dict): Dictionary containing time and real-time\n",
    "            factor metrics for processing.\n",
    "    \"\"\"\n",
    "    # Convert text to tokens\n",
    "    st = time.time()\n",
    "\n",
    "    tokens = tokenizer.texts_to_token_ids([text])\n",
    "    prompt_tokens = tokenizer.texts_to_token_ids([prompt_text])\n",
    "\n",
    "    # Load and preprocess prompt wav\n",
    "    prompt_wav, prompt_sampling_rate = torchaudio.load(prompt_wav)\n",
    "\n",
    "    if prompt_sampling_rate != sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=prompt_sampling_rate, new_freq=sampling_rate\n",
    "        )\n",
    "        prompt_wav = resampler(prompt_wav)\n",
    "\n",
    "    prompt_rms = torch.sqrt(torch.mean(torch.square(prompt_wav)))\n",
    "    if prompt_rms < target_rms:\n",
    "        prompt_wav = prompt_wav * target_rms / prompt_rms\n",
    "\n",
    "    # Extract features from prompt wav\n",
    "    prompt_features = feature_extractor.extract(\n",
    "        prompt_wav, sampling_rate=sampling_rate\n",
    "    ).to(device)\n",
    "\n",
    "    prompt_features = prompt_features.unsqueeze(0) * feat_scale\n",
    "    prompt_features_lens = torch.tensor([prompt_features.size(1)], device=device)\n",
    "\n",
    "    set_time = time.time() - st\n",
    "\n",
    "    # Start timing\n",
    "    start_t = dt.datetime.now() # ì•½ 5%\n",
    "\n",
    "    # Generate features\n",
    "    (\n",
    "        pred_features,\n",
    "        pred_features_lens,\n",
    "        pred_prompt_features,\n",
    "        pred_prompt_features_lens,\n",
    "    ) = model.sample(\n",
    "        tokens=tokens,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        prompt_features=prompt_features,\n",
    "        prompt_features_lens=prompt_features_lens,\n",
    "        speed=speed,\n",
    "        t_shift=t_shift,\n",
    "        duration=\"predict\",\n",
    "        num_step=num_step,\n",
    "        guidance_scale=guidance_scale,\n",
    "    )\n",
    "\n",
    "    # Postprocess predicted features\n",
    "    pred_features = pred_features.permute(0, 2, 1) / feat_scale  # (B, C, T)\n",
    "\n",
    "    # Start vocoder processing\n",
    "    start_vocoder_t = dt.datetime.now()\n",
    "    wav = vocoder.decode(pred_features).squeeze(1).clamp(-1, 1)\n",
    "\n",
    "    # Calculate processing times and real-time factors\n",
    "    t = (dt.datetime.now() - start_t).total_seconds()\n",
    "    t_no_vocoder = (start_vocoder_t - start_t).total_seconds()\n",
    "    t_vocoder = (dt.datetime.now() - start_vocoder_t).total_seconds()\n",
    "    wav_seconds = wav.shape[-1] / sampling_rate\n",
    "    rtf = t / wav_seconds\n",
    "    rtf_no_vocoder = t_no_vocoder / wav_seconds\n",
    "    rtf_vocoder = t_vocoder / wav_seconds\n",
    "    metrics = {\n",
    "        \"t\": t,\n",
    "        \"t_no_vocoder\": t_no_vocoder,\n",
    "        \"t_vocoder\": t_vocoder,\n",
    "        \"wav_seconds\": wav_seconds,\n",
    "        \"rtf\": rtf,\n",
    "        \"rtf_no_vocoder\": rtf_no_vocoder,\n",
    "        \"rtf_vocoder\": rtf_vocoder,\n",
    "        \"set_time\": set_time\n",
    "    }\n",
    "\n",
    "    # Adjust wav volume if necessary\n",
    "    if prompt_rms < target_rms:\n",
    "        wav = wav * prompt_rms / target_rms\n",
    "    # torchaudio.save(save_path, wav.cpu(), sample_rate=sampling_rate)\n",
    "\n",
    "    return wav.cpu(), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a62a184-130b-4b54-bc25-cfd095c34061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "default_args = {\n",
    "    \"model_name\": \"zipvoice_distill\",\n",
    "    \"model_dir\": None,\n",
    "    \"checkpoint_name\": \"model.pt\",\n",
    "    \"vocoder_path\": None,\n",
    "    \"target_rms\": 0.1,\n",
    "    \"tokenizer\": \"emilia\",\n",
    "    \"lang\": \"en-us\",\n",
    "    \"test_list\": None,\n",
    "    \"prompt_wav\": None,\n",
    "    \"prompt_text\": None,\n",
    "    \"text\": None,\n",
    "    \"res_dir\": \"results\",\n",
    "    \"res_wav_path\": \"result.wav\",\n",
    "    \"guidance_scale\": None,\n",
    "    \"num_step\": None,\n",
    "    \"feat_scale\": 0.1,\n",
    "    \"speed\": 1.0,\n",
    "    \"seed\": 111,\n",
    "    \"t_shift\": 0.5\n",
    "}\n",
    "params = Namespace(**default_args)\n",
    "params.prompt_wav = \"/workspace/ttssocketserver/tts/voice.wav\"\n",
    "params.prompt_text = \"This is a test.\"\n",
    "params.text = \"Tell me anything just do it!!!\"\n",
    "\n",
    "fix_random_seed(params.seed)\n",
    "\n",
    "model_defaults = {\n",
    "    \"zipvoice\": {\n",
    "        \"num_step\": 16,\n",
    "        \"guidance_scale\": 1.0,\n",
    "    },\n",
    "    \"zipvoice_distill\": {\n",
    "        \"num_step\": 8,\n",
    "        \"guidance_scale\": 3.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_specific_defaults = model_defaults.get(params.model_name, {})\n",
    "\n",
    "for param, value in model_specific_defaults.items():\n",
    "    if getattr(params, param) is None:\n",
    "        setattr(params, param, value)\n",
    "        logging.info(f\"Setting {param} to default value: {value}\")\n",
    "\n",
    "assert (params.test_list is not None) ^ (\n",
    "    (params.prompt_wav and params.prompt_text and params.text) is not None\n",
    "), (\n",
    "    \"For inference, please provide prompts and text with either '--test-list'\"\n",
    "    \" or '--prompt-wav, --prompt-text and --text'.\"\n",
    ")\n",
    "\n",
    "if params.model_dir is not None:\n",
    "    params.model_dir = Path(params.model_dir)\n",
    "    if not params.model_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"{params.model_dir} does not exist\")\n",
    "    for filename in [params.checkpoint_name, \"model.json\", \"tokens.txt\"]:\n",
    "        if not (params.model_dir / filename).is_file():\n",
    "            raise FileNotFoundError(f\"{params.model_dir / filename} does not exist\")\n",
    "    model_ckpt = params.model_dir / params.checkpoint_name\n",
    "    model_config = params.model_dir / \"model.json\"\n",
    "    token_file = params.model_dir / \"tokens.txt\"\n",
    "    logging.info(\n",
    "        f\"Using local model dir {params.model_dir}, \"\n",
    "        f\"checkpoint {params.checkpoint_name}\"\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"Using pretrained model from the huggingface\")\n",
    "    logging.info(\"Downloading the requires files from HuggingFace\")\n",
    "    model_ckpt = hf_hub_download(\n",
    "        HUGGINGFACE_REPO, filename=f\"{MODEL_DIR[params.model_name]}/model.pt\"\n",
    "    )\n",
    "    model_config = hf_hub_download(\n",
    "        HUGGINGFACE_REPO, filename=f\"{MODEL_DIR[params.model_name]}/model.json\"\n",
    "    )\n",
    "\n",
    "    token_file = hf_hub_download(\n",
    "        HUGGINGFACE_REPO, filename=f\"{MODEL_DIR[params.model_name]}/tokens.txt\"\n",
    "    )\n",
    "\n",
    "logging.info(\"Loading model...\")\n",
    "\n",
    "if params.tokenizer == \"emilia\":\n",
    "    tokenizer = EmiliaTokenizer(token_file=token_file)\n",
    "elif params.tokenizer == \"libritts\":\n",
    "    tokenizer = LibriTTSTokenizer(token_file=token_file)\n",
    "elif params.tokenizer == \"espeak\":\n",
    "    tokenizer = EspeakTokenizer(token_file=token_file, lang=params.lang)\n",
    "else:\n",
    "    assert params.tokenizer == \"simple\"\n",
    "    tokenizer = SimpleTokenizer(token_file=token_file)\n",
    "\n",
    "tokenizer_config = {\"vocab_size\": tokenizer.vocab_size, \"pad_id\": tokenizer.pad_id}\n",
    "\n",
    "with open(model_config, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "if params.model_name == \"zipvoice\":\n",
    "    model = ZipVoice(\n",
    "        **model_config[\"model\"],\n",
    "        **tokenizer_config,\n",
    "    )\n",
    "else:\n",
    "    assert params.model_name == \"zipvoice_distill\"\n",
    "    model = ZipVoiceDistill(\n",
    "        **model_config[\"model\"],\n",
    "        **tokenizer_config,\n",
    "    )\n",
    "\n",
    "if str(model_ckpt).endswith(\".safetensors\"):\n",
    "    safetensors.torch.load_model(model, model_ckpt)\n",
    "elif str(model_ckpt).endswith(\".pt\"):\n",
    "    load_checkpoint(filename=model_ckpt, model=model, strict=True)\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unsupported model checkpoint format: {model_ckpt}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    params.device = torch.device(\"cuda\", 0)\n",
    "elif torch.backends.mps.is_available():\n",
    "    params.device = torch.device(\"mps\")\n",
    "else:\n",
    "    params.device = torch.device(\"cpu\")\n",
    "logging.info(f\"Device: {params.device}\")\n",
    "\n",
    "model = model.to(params.device)\n",
    "model.eval()\n",
    "\n",
    "vocoder = get_vocoder(params.vocoder_path)\n",
    "vocoder = vocoder.to(params.device)\n",
    "vocoder.eval()\n",
    "\n",
    "if model_config[\"feature\"][\"type\"] == \"vocos\":\n",
    "    feature_extractor = VocosFbank()\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        f\"Unsupported feature type: {model_config['feature']['type']}\"\n",
    "    )\n",
    "\n",
    "params.sampling_rate = model_config[\"feature\"][\"sampling_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64acafa-126c-4ec2-a94f-fde9c263097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Duration prediction : 0.13699555397033691\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Only solver : 0.7648537158966064\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total : 0.9077918529510498\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
     ]
    }
   ],
   "source": [
    "waveform, metrics = generate_sentence(\n",
    "    save_path=params.res_wav_path,\n",
    "    prompt_text=params.prompt_text,\n",
    "    prompt_wav=params.prompt_wav,\n",
    "    text=params.text,\n",
    "    model=model,\n",
    "    vocoder=vocoder,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=params.device,\n",
    "    num_step=params.num_step,\n",
    "    guidance_scale=params.guidance_scale,\n",
    "    speed=params.speed,\n",
    "    t_shift=params.t_shift,\n",
    "    target_rms=params.target_rms,\n",
    "    feat_scale=params.feat_scale,\n",
    "    sampling_rate=params.sampling_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083d707-e4ff-4e0a-bb3a-3aa94a3695f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9561bf-2baa-43c8-8a2d-b0946c3a9296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b162d1-901d-45ca-b431-1f5faec32de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d5f6a-41c2-4166-984d-46af318fc92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be1c22-0169-40a4-ac0d-7ba259083edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b278d13-c218-4cd2-9004-589ee5a983e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
