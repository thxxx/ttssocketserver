{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0812f6-3b02-42f7-8d12-e9785e8592f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"ghost613/whisper-large-v3-turbo-korean\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93bb975-de67-4ac5-8a01-1ffaa19fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "koprocessor = AutoProcessor.from_pretrained(\"o0dimplz0o/Whisper-Large-v3-turbo-STT-Zeroth-KO-v2\")\n",
    "komodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"o0dimplz0o/Whisper-Large-v3-turbo-STT-Zeroth-KO-v2\")\n",
    "\n",
    "komodel.save_pretrained(\"/workspace/o0dimplz0o_zeroth_ko_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df385223-2f7c-47fc-832b-9767356c0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=komodel,\n",
    "    tokenizer=koprocessor.tokenizer,\n",
    "    feature_extractor=koprocessor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b420af-6f72-41d0-a608-911a8cb5d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.055109977722168\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "import time\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# base64 문자열 예시\n",
    "with open(\"eo_05.mp3\", \"rb\") as f:\n",
    "    b64_audio = base64.b64encode(f.read()).decode()\n",
    "\n",
    "audio_bytes = base64.b64decode(b64_audio)\n",
    "stt = time.time()\n",
    "data, sr = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\", always_2d=True)\n",
    "mono = data.mean(axis=1)  # 모노 변환\n",
    "mono_16k = librosa.resample(mono, orig_sr=sr, target_sr=16000)\n",
    "print(time.time() - stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c845e606-d9fe-471b-b8b6-b2e46075f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '그런 얘기들 되게 많이 하시는데 정석을 급히 작겠어요'} \n",
      " 0.40485095977783203\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "params = dict(\n",
    "    beam_size=1,                # 1이면 그리디; 빔서치(>=2)는 느림. 보통 1~2 추천\n",
    "    best_of=1,                  # 그리디일 때는 의미 없음. 작게.\n",
    "    temperature=0.0,            # 안정/속도↑. (불확실 시 fallback으로 [0.0, 0.2, 0.4] 등 가능)\n",
    "    vad_filter=True,            # 무음 제거 → 디코딩량 ↓\n",
    "    vad_parameters=dict(min_silence_duration_ms=500),\n",
    "    chunk_length=15,            # 초 단위. 10~20초 권장(긴 문맥 필요하면 20~30)\n",
    "    no_speech_threshold=0.6,    # 무음 판단 강화로 낭비 줄이기\n",
    "    log_prob_threshold=-1.0,    # 너무 낮으면 실패로 간주하고 temperature fallback 고려\n",
    "    word_timestamps=False,      # 단어 타임스탬프 끄면 속도↑ (필요 시만 True)\n",
    "    condition_on_previous_text=False,  # 청크 간 의존 줄여 속도/안정 ↑\n",
    ")\n",
    "st = time.time()\n",
    "result = pipe(\n",
    "    mono_16k,\n",
    "    generate_kwargs={\"language\":\"korean\"}\n",
    ")\n",
    "print(result, \"\\n\", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add33b02-8e0f-477e-80ac-9ad6075ab045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(input_features: Optional[torch.FloatTensor] = None, attention_mask: Optional[torch.LongTensor] = None, decoder_input_ids: Optional[torch.LongTensor] = None, decoder_attention_mask: Optional[torch.LongTensor] = None, head_mask: Optional[torch.Tensor] = None, decoder_head_mask: Optional[torch.Tensor] = None, cross_attn_head_mask: Optional[torch.Tensor] = None, encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None, past_key_values: Optional[transformers.cache_utils.Cache] = None, decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None, decoder_position_ids: Optional[tuple[torch.LongTensor]] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, cache_position: Optional[torch.LongTensor] = None) -> Union[tuple[torch.Tensor], transformers.modeling_outputs.Seq2SeqLMOutput]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1285bdae-224c-4131-9f4d-da92fb87ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 완료: output.mp3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc8064-6d12-4b25-9e16-404961050a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981abac-5498-4240-a835-0a22049e0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"o0dimplz0o/Zeroth-STT-Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c093c9-a6c8-461f-9855-51773f51e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c29c83d-8254-41de-ba99-0f1504db99bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e5cfa-e69a-4583-bbf8-b6279674cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500c9616-4e5b-48a7-b832-43159a5376e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-19 01:46:35 nemo_logging:393] Tokenizer CanaryBPETokenizer initialized with 16384 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-19 01:46:35 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    prompt_format: canary2\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.01\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: true\n",
      "    max_tps: null\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: null\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-08-19 01:46:35 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    prompt_format: canary2\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-19 01:46:35 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-08-19 01:46:47 nemo_logging:393] Model EncDecMultiTaskModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--canary-1b-v2/snapshots/efbc7e5840703084f201cd086cf936c02d363b22/canary-1b-v2.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(\"nvidia/canary-1b-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bc2e8c-d0b4-48d6-9df5-3dbb2acf0673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They walked hand in hand back to their village, the rising sun illuminating their path. The villagers, who had long mourned Finn, gasped in disbelief, then erupted in joyous cries. Elara had not only found her brother, but she had also unveiled a truth long hidden, proving that even in the face of the unknown, hope and an unyielding will could bridge the impossible. The veiled realm remained, a mystery just beyond their perception, but for Elara and Finn, the most important journey had been the one back home. \n",
      " 1.8883419036865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "import time\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "# # base64 문자열 예시\n",
    "# with open(\"eo_05.mp3\", \"rb\") as f:\n",
    "#     b64_audio = base64.b64encode(f.read()).decode()\n",
    "\n",
    "# audio_bytes = base64.b64decode(b64_audio)\n",
    "# data, sr = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\", always_2d=True)\n",
    "# mono = data.mean(axis=1)  # 모노 변환\n",
    "# mono_16k = librosa.resample(mono, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "audio, sr = librosa.load('talks.mp3', sr=16000, mono=True)\n",
    "st = time.time()\n",
    "transcriptions = asr_model.transcribe([audio])\n",
    "cleaned = re.sub(r\"<\\|.*?\\|>\", \"\", transcriptions[0].text).strip()\n",
    "print(cleaned, \"\\n\", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba28e08e-3319-47b9-a976-a753079cf6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hypothesis(score=0.0, y_sequence=tensor([   16,    64,     5,     9,    11,    13,  6103,  9538,  3908,  3327,\n",
       "          1220,  3327,  6004,  1237,  3755,  5130,  2472, 16067,  1289,  3324,\n",
       "          1260,  8218,  7766,  1276,  1170,  9403,  3755,  2118, 16071, 16073,\n",
       "          1839,  5130,  1264,  1385, 16067,  3602,  2421,  5639,  1510,  3830,\n",
       "          1212,  7184, 16067, 13685, 14437,  1220,  1857,  6750,  6184, 16067,\n",
       "          4551,  1359,  2128,  2960,  1220,  1783, 16081,  3636,  6937,  1172,\n",
       "         16073,  2445,  1781,  2421,  2019,  5397,  9633,  2173,  4033, 16058,\n",
       "          1871, 16067,  2867,  3930,  2421,  2935,  1243,  1395,  1194,  1212,\n",
       "          1168,  1434,  6224,  5639, 13004,  1598, 16067,  5939,  1260,  1644,\n",
       "          4499,  1220,  1289,  4314,  1381,  1289,  1243, 16066,  1358,  2210,\n",
       "         16067, 14777,  1392,  1274,  1243, 16081, 16056,  2319,  1260,  2735,\n",
       "          4972,  2099, 10312,  1289,  2062,  1198,  1423,  2655, 16073,  1839,\n",
       "          1501,  1194,  1212,  4275, 16065, 12700, 10088, 16067,  1168,  1904,\n",
       "          4615, 16081,  3506,  1414, 16081,  1700,  3755,  1474,  3670,  1995,\n",
       "         16067,  2867,  1378,  2445,  1781,  1392,  7184, 16067,  1289,  3667,\n",
       "          5823, 10019,  7206,  2421,  4292,  1289,  2825,  6004, 12091, 16073]), text='<|emo:undefined|><|en|><|pnc|><|noitn|><|notimestamp|><|nodiarize|> They walked hand in hand back to their village, the rising sun illuminating their path. The villagers, who had long mourned Finn, gasped in disbelief, then erupted in joyous cries. Elara had not only found her brother, but she had also unveiled a truth long hidden, proving that even in the face of the unknown, hope and an unyielding will could bridge the impossible. The veiled realm remained, a mystery just beyond their perception, but for Elara and Finn, the most important journey had been the one back home.', dec_out=None, dec_state=None, timestamp={'word': [], 'segment': [], 'char': []}, alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf808e-5e26-4301-a1f7-9544ad6a1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "transcriptions = asr_model.transcribe([\"eo_05.mp3\"])\n",
    "print(transcriptions, \"\\n\", time.time() - st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
