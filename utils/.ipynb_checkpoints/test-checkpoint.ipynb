{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c71d07d-c101-4af9-8e65-74ae47dfbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return new[:i] + new[i:], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61d3524-2bd3-486f-89d0-c7eea4fa1bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1484832763671875e-05\n"
     ]
    }
   ],
   "source": [
    "old = \"From Now on, we we will try to fix it.\"\n",
    "new = \"From now on We will try to fix it, okay?\"\n",
    "\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "text_pr(old, new)\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127da5fc-af40-4f7d-b5a3-7730ca32ea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 09:46:30 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-02 09:46:31 [utils.py:326] non-default args: {'model': 'ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 512, 'disable_log_stats': True}\n",
      "INFO 09-02 09:46:38 [__init__.py:711] Resolved architecture: MistralForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-02 09:46:38 [__init__.py:1750] Using max model len 32768\n",
      "INFO 09-02 09:46:39 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:40 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:40 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', speculative_config=None, tokenizer='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:41 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m WARNING 09-02 09:46:41 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:41 [gpu_model_runner.py:1953] Starting to load model ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8...\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:41 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:41 [compressed_tensors_w8a8_int8.py:52] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:42 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:42 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595c26ca104040499b395ddf04f8f6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca378dd2e824315988e5708a72110f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:51 [weight_utils.py:312] Time spent downloading weights for ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8: 9.150977 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d457a2a2c10f42d89293981a683f35b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3919981950744b9f8362433f65ecedfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:56 [default_loader.py:262] Loading weights took 4.80 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:46:57 [gpu_model_runner.py:2007] Model loading took 7.5096 GiB and 15.065258 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:05 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/09a4ce9418/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:05 [backends.py:559] Dynamo bytecode transform time: 8.16 s\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:08 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:31 [backends.py:215] Compiling a graph for dynamic shape takes 25.16 s\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:47 [monitor.py:34] torch.compile takes 33.32 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:48 [gpu_worker.py:276] Available KV cache memory: 6.09 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:48 [kv_cache_utils.py:849] GPU KV cache size: 49,920 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:48 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 4.06x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:06<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:55 [gpu_model_runner.py:2708] Graph capturing finished in 7 secs, took 0.58 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=8003)\u001b[0;0m INFO 09-02 09:47:55 [core.py:214] init engine (profile, create kv cache, warmup model) took 57.98 seconds\n",
      "INFO 09-02 09:47:56 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "model_path = \"ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8\"\n",
    "model = LLM(model=model_path,\n",
    "            max_num_seqs=512,\n",
    "            tensor_parallel_size=1,\n",
    "            enable_prefix_caching=True, \n",
    "            gpu_memory_utilization=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f465ae0c-f62b-4ed3-88bb-eb2d424e8af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4fe769b0184ba59795ce84e6ee9d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d659a858052401c9d87d3f40e9eef22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369097232818604\n",
      "['연방준비은행(Fed)은 사실 미']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "messages = [\n",
    "    # without CoT\n",
    "    \"Translate the following English sentence into Korean:\\nThe Fed is actually not just the central bank of the United States, but it controls many other banks.<ko>\",\n",
    "    # with CoT\n",
    "    # \"Translate the following English sentence into Korean and explain it in detail:\\nMay the force be with you <ko>\" \n",
    "]\n",
    "\n",
    "# Beam search (We recommend using beam search decoding)\n",
    "# decoding_params = BeamSearchParams(beam_width=4,\n",
    "#                                    max_tokens=512)\n",
    "# Greedy decoding\n",
    "decoding_params = SamplingParams(temperature=0.2,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "st = time.time()\n",
    "results = model.generate(messages)\n",
    "print(time.time() - st)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efd889-52ef-4247-8405-2a0a272f3dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
