{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0812f6-3b02-42f7-8d12-e9785e8592f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"ghost613/whisper-large-v3-turbo-korean\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93bb975-de67-4ac5-8a01-1ffaa19fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "koprocessor = AutoProcessor.from_pretrained(\"o0dimplz0o/Whisper-Large-v3-turbo-STT-Zeroth-KO-v2\")\n",
    "komodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"o0dimplz0o/Whisper-Large-v3-turbo-STT-Zeroth-KO-v2\")\n",
    "\n",
    "komodel.save_pretrained(\"/workspace/o0dimplz0o_zeroth_ko_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df385223-2f7c-47fc-832b-9767356c0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=komodel,\n",
    "    tokenizer=koprocessor.tokenizer,\n",
    "    feature_extractor=koprocessor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b420af-6f72-41d0-a608-911a8cb5d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.055109977722168\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "import time\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# base64 문자열 예시\n",
    "with open(\"eo_05.mp3\", \"rb\") as f:\n",
    "    b64_audio = base64.b64encode(f.read()).decode()\n",
    "\n",
    "audio_bytes = base64.b64decode(b64_audio)\n",
    "stt = time.time()\n",
    "data, sr = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\", always_2d=True)\n",
    "mono = data.mean(axis=1)  # 모노 변환\n",
    "mono_16k = librosa.resample(mono, orig_sr=sr, target_sr=16000)\n",
    "print(time.time() - stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c845e606-d9fe-471b-b8b6-b2e46075f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '그런 얘기들 되게 많이 하시는데 정석을 급히 작겠어요'} \n",
      " 0.40485095977783203\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "params = dict(\n",
    "    beam_size=1,                # 1이면 그리디; 빔서치(>=2)는 느림. 보통 1~2 추천\n",
    "    best_of=1,                  # 그리디일 때는 의미 없음. 작게.\n",
    "    temperature=0.0,            # 안정/속도↑. (불확실 시 fallback으로 [0.0, 0.2, 0.4] 등 가능)\n",
    "    vad_filter=True,            # 무음 제거 → 디코딩량 ↓\n",
    "    vad_parameters=dict(min_silence_duration_ms=500),\n",
    "    chunk_length=15,            # 초 단위. 10~20초 권장(긴 문맥 필요하면 20~30)\n",
    "    no_speech_threshold=0.6,    # 무음 판단 강화로 낭비 줄이기\n",
    "    log_prob_threshold=-1.0,    # 너무 낮으면 실패로 간주하고 temperature fallback 고려\n",
    "    word_timestamps=False,      # 단어 타임스탬프 끄면 속도↑ (필요 시만 True)\n",
    "    condition_on_previous_text=False,  # 청크 간 의존 줄여 속도/안정 ↑\n",
    ")\n",
    "st = time.time()\n",
    "result = pipe(\n",
    "    mono_16k,\n",
    "    generate_kwargs={\"language\":\"korean\"}\n",
    ")\n",
    "print(result, \"\\n\", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add33b02-8e0f-477e-80ac-9ad6075ab045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(input_features: Optional[torch.FloatTensor] = None, attention_mask: Optional[torch.LongTensor] = None, decoder_input_ids: Optional[torch.LongTensor] = None, decoder_attention_mask: Optional[torch.LongTensor] = None, head_mask: Optional[torch.Tensor] = None, decoder_head_mask: Optional[torch.Tensor] = None, cross_attn_head_mask: Optional[torch.Tensor] = None, encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None, past_key_values: Optional[transformers.cache_utils.Cache] = None, decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None, decoder_position_ids: Optional[tuple[torch.LongTensor]] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, cache_position: Optional[torch.LongTensor] = None) -> Union[tuple[torch.Tensor], transformers.modeling_outputs.Seq2SeqLMOutput]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.signature(model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1285bdae-224c-4131-9f4d-da92fb87ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 완료: output.mp3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc8064-6d12-4b25-9e16-404961050a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981abac-5498-4240-a835-0a22049e0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"o0dimplz0o/Zeroth-STT-Korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c093c9-a6c8-461f-9855-51773f51e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = set(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c29c83d-8254-41de-ba99-0f1504db99bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e5cfa-e69a-4583-bbf8-b6279674cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c9616-4e5b-48a7-b832-43159a5376e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(\"nvidia/canary-1b-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc2e8c-d0b4-48d6-9df5-3dbb2acf0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import time\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import re\n",
    "\n",
    "# # base64 문자열 예시\n",
    "# with open(\"eo_05.mp3\", \"rb\") as f:\n",
    "#     b64_audio = base64.b64encode(f.read()).decode()\n",
    "\n",
    "# audio_bytes = base64.b64decode(b64_audio)\n",
    "# data, sr = sf.read(io.BytesIO(audio_bytes), dtype=\"float32\", always_2d=True)\n",
    "# mono = data.mean(axis=1)  # 모노 변환\n",
    "# mono_16k = librosa.resample(mono, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "audio, sr = librosa.load('talks.mp3', sr=16000, mono=True)\n",
    "st = time.time()\n",
    "transcriptions = asr_model.transcribe([audio])\n",
    "cleaned = re.sub(r\"<\\|.*?\\|>\", \"\", transcriptions[0].text).strip()\n",
    "print(cleaned, \"\\n\", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf808e-5e26-4301-a1f7-9544ad6a1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "transcriptions = asr_model.transcribe([\"eo_05.mp3\"])\n",
    "print(transcriptions, \"\\n\", time.time() - st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
