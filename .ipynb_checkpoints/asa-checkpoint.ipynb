{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937171f-dcb7-40a2-b1a8-c6934454bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.tts import ChatterboxTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b990dce-7e20-4093-bf59-7d0b82345a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "text = \"hm..\"\n",
    "out = 'hmhm.wav'\n",
    "wav, tokens = model.generate(text, audio_prompt_path='/workspace/chatterbox/sesame.wav')\n",
    "print(wav.shape[-1]/24000)\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974524-6487-4a4d-9ff4-0d879babc25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a8a5-995b-479d-8aac-8f892bc9f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import torch\n",
    "\n",
    "display(Audio(wav, rate=24000))\n",
    "bi=0\n",
    "all_audios = []\n",
    "for i in [20, 40, 60, 80, 100, 120, 140, 160]:\n",
    "    cwav, _ = model.s3gen.inference(\n",
    "        speech_tokens=tokens[bi:i],\n",
    "        ref_dict=model.conds.gen,\n",
    "    )\n",
    "    all_audios.append(cwav.detach().cpu())\n",
    "    # display(Audio(cwav.detach().cpu(), rate=24000))\n",
    "    bi = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a1c8f-40c3-4aa4-8045-7b70b1ac8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_wav = torch.concat(all_audios, dim=1)\n",
    "display(Audio(summed_wav, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b900ad6-e1a7-4205-86c3-761bc79170ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec883bfc-f945-4ec5-8ca8-c38d8263081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfc614-8a79-4b4a-a9a2-89dee073d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e238b8b-55d6-406c-be8c-d8e7bf50cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox.tts import ChatterboxTTS\n",
    "import time\n",
    "from IPython.display import Audio\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d950d20-fc13-41af-b505-b5d7a0dd45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0\n",
    "st = time.time()\n",
    "wavs = []\n",
    "async for evt in model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]\n",
    "        wavs.append(wav[:, max(0, last_length-1000):])\n",
    "        print(f\"[{(wav.shape[-1]-last_length)/24000}s] - {time.time() - st}\")\n",
    "        last_length = wav.shape[-1]\n",
    "        st = time.time()\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d6f40-0423-43ec-bfdc-8ede80c9346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "\n",
    "SR = 24000\n",
    "OVERLAP = int(0.05 * SR)  # 50ms\n",
    "\n",
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0          # 모델이 지금까지 만든 전체 wav 길이\n",
    "last_tail = None         # 직전 출력 청크의 꼬리(OVERLAP 샘플)\n",
    "st = time.time()\n",
    "\n",
    "wavs = []                # overlap-add로 섞은 최종 출력 청크들\n",
    "\n",
    "async for evt in tts_model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]                    # shape: (ch, T_total_so_far)\n",
    "        device = wav.device\n",
    "        dtype = wav.dtype\n",
    "\n",
    "        # 이번에 \"새로 추가된\" 구간만 잘라오기\n",
    "        new_total = wav.shape[-1]\n",
    "        delta = new_total - last_length\n",
    "        if delta <= 0:\n",
    "            continue  # 새로 생긴 게 없으면 스킵\n",
    "\n",
    "        new_part = wav[:, last_length:new_total]  # (ch, delta)\n",
    "\n",
    "        if last_tail is None:\n",
    "            # 첫 청크면 바로 내보냄\n",
    "            out_chunk = new_part\n",
    "        else:\n",
    "            # 겹치는 길이 L (= min(OVERLAP, 새로 생긴 길이, last_tail 길이))\n",
    "            L = min(OVERLAP, new_part.shape[-1], last_tail.shape[-1])\n",
    "            if L > 0:\n",
    "                # last_tail의 마지막 L 샘플 ↔ new_part의 앞 L 샘플을 교차페이드\n",
    "                fade_in  = torch.linspace(0, 1, L, device=device, dtype=dtype)\n",
    "                fade_out = 1.0 - fade_in\n",
    "\n",
    "                mixed = last_tail[:, -L:] * fade_out + new_part[:, :L] * fade_in\n",
    "                tail  = new_part[:, L:]  # 비겹침 뒷부분\n",
    "\n",
    "                out_chunk = torch.cat([mixed, tail], dim=-1)\n",
    "            else:\n",
    "                # 겹칠 게 없으면 그냥 이어붙임\n",
    "                out_chunk = new_part\n",
    "\n",
    "        # 다음 교차페이드를 위해 꼬리 갱신\n",
    "        # (모델 기준 전체 wav의 최신 꼬리를 쓰는 게 안전)\n",
    "        new_tail_start = max(0, new_total - OVERLAP)\n",
    "        last_tail = wav[:, new_tail_start:new_total].detach()\n",
    "\n",
    "        # 사용자 출력/저장을 위해 overlap-add 결과만 모음\n",
    "        wavs.append(out_chunk)\n",
    "\n",
    "        print(f\"[{out_chunk.shape[-1]/SR:.3f}s] - {time.time() - st:.3f}\")\n",
    "        last_length = new_total\n",
    "        st = time.time()\n",
    "\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7ddd2-1d24-4789-b64d-f9030c6cf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "display(Audio(wav.cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수\n",
    "display(Audio(torch.concat(wavs, dim=-1).cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db3e6d-cd50-4d8c-b143-08685d4d5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "model.t3.speech_pos_emb.get_fixed_embedding(213)\n",
    "time.time() - st\n",
    "\n",
    "import torch\n",
    "\n",
    "st = time.time()\n",
    "emb = torch.nn.Embedding(113, 512)\n",
    "emb(torch.tensor(12))\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b655e-a453-4e9b-936d-4527fbb0db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "torch.tensor(132).to('cuda')\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451018f-2fff-4cd7-b614-124dbd15b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn((132001))\n",
    "st = time.time()\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "dist = torch.distributions.Categorical(logits=logits)  # GPU에서 동작\n",
    "next_token = dist.sample()  # (B, 1) 형태 맞추기\n",
    "print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad1e0-7cc3-41a4-98d4-967872a0ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "\n",
    "audio, sr = librosa.load('test-1.wav')\n",
    "\n",
    "text = \"Oh\"\n",
    "st = time.time()\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape, tokens.shape, time.time() - st)\n",
    "# ta.save(\"test-1.wav\", wav, model.sr)\n",
    "display(Audio('test-1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75654aa5-03ac-4e15-a31e-86bba64bf8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7c396-b929-446a-b66f-15d92124f731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c7f32-c788-4acc-9716-8418c9ea84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.tts import ChatterboxTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe3330-1b9a-437b-8bd0-b343054a5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "audio, _ = librosa.load('/workspace/chatterbox/sesame.wav', sr=16000, mono=True)\n",
    "\n",
    "text = \"I want to go home now please let me go.\"\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape[-1]/24000)\n",
    "\n",
    "out = 'just.wav'\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cb1c0-3dfd-4453-9ac1-bec435ed4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d81fc708-1174-452c-a9f3-eaf8f1a445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "aa = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f71270c-f7fe-45c8-861e-f02186d0e2d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43maa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "aa.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b822a95-ce59-4cda-a085-647a101d46b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
