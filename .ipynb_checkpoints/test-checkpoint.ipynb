{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583911-fbbb-4604-a73f-cef3df232bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"trillionlabs/Tri-1.8B-Translation\",\n",
    "    dtype=\"float16\",               # GPU라면 권장\n",
    "    tensor_parallel_size=1,        # 단일 GPU 보장\n",
    "    enforce_eager=True,            # CUDA graph 캡쳐 이슈 회피\n",
    "    gpu_memory_utilization=0.5,   # 메모리 여유 확보\n",
    "    disable_log_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618c23d-68ef-4d74-80ba-ea2bd31c61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SamplingParams(temperature=0.1, max_tokens=512)\n",
    "\n",
    "target = \"ko\"\n",
    "text = \"\"\"There's so much to do on a day like this, lots of things to do, but maybe later when you're having dinner.\"\"\"\n",
    "\n",
    "prompt_old = f\"\"\"\n",
    "Translate into ko\\n\n",
    "{text}<ko>\n",
    "이런 날에는 할 일이 너무 많고 할 일도 많지만, 나중에 저녁을 먹을 때쯤이면.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate into ko\\n\n",
    "There will be a chance to reflect.<ko>\n",
    "\"\"\"\n",
    "out = llm.chat([{\"role\": \"user\", \"content\": prompt}], sampling_params=sp)\n",
    "out[0].outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004363e-f612-4b01-b993-37aa3a17c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a137b4-10f7-499f-b0ba-084fb76867d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd8b09e-7a71-4095-8895-a74c88ed9d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-03 03:08:43 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-03 03:08:44 [utils.py:326] non-default args: {'model': 'ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', 'enable_prefix_caching': True, 'max_num_seqs': 512, 'disable_log_stats': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a410c7a3e4402483c290e72268b818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-03 03:08:51 [__init__.py:711] Resolved architecture: MistralForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-03 03:08:51 [__init__.py:1750] Using max model len 32768\n",
      "INFO 09-03 03:08:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9546bf33004cb7bd634b1f48fbe257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/906 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93255becfe6a4baca7c584f1db320076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c348cbe6324fc5b2e7ce61986355c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aa470aa47b4539bab8c032ee6c8ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:55 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:55 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', speculative_config=None, tokenizer='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m WARNING 09-03 03:08:56 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:56 [gpu_model_runner.py:1953] Starting to load model ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8...\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:56 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:56 [compressed_tensors_w8a8_int8.py:52] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:56 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:08:57 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e07fb2b3f846668888168885d0c123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd13be0cc36946d38cc30f27feb3b76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:06 [weight_utils.py:312] Time spent downloading weights for ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8: 9.334460 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d26d0fec4342bdac427b8fe76acda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6f8644976b4b909da64864167d8d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:27 [default_loader.py:262] Loading weights took 21.35 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:28 [gpu_model_runner.py:2007] Model loading took 7.5096 GiB and 31.442763 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/09a4ce9418/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:36 [backends.py:559] Dynamo bytecode transform time: 8.15 s\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:09:39 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:02 [backends.py:215] Compiling a graph for dynamic shape takes 25.17 s\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:17 [monitor.py:34] torch.compile takes 33.32 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:19 [gpu_worker.py:276] Available KV cache memory: 5.31 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:19 [kv_cache_utils.py:849] GPU KV cache size: 43,488 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:19 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 3.53x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:06<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:26 [gpu_model_runner.py:2708] Graph capturing finished in 7 secs, took 0.58 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=7013)\u001b[0;0m INFO 09-03 03:10:26 [core.py:214] init engine (profile, create kv cache, warmup model) took 58.13 seconds\n",
      "INFO 09-03 03:10:27 [llm.py:298] Supported_tasks: ['generate']\n",
      "ERROR 09-03 03:15:11 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    max_num_seqs=512,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=True, \n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26aa1425-7cf4-4207-86f8-329facfb5cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d0a47d741045b49340c246f0a95519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2568085ff8a947168350cac8bcc6f221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어, 어린 시절의 꿈이었어요.']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치...\"\"\",\n",
    "]\n",
    "\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0.1,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "results = model.generate(messages, decoding_params)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de11b2-4c90-4f5d-a1b3-0acfffb7d1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27eeba-f2b0-4041-8502-74c81b321c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd79cea9-f208-4610-a4ec-ceb25d84c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4449949264526367 \n",
      " ChatCompletion(id='chatcmpl-CBYFexjpVJTh5561UskZKIjqGItv8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869306, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.6492297649383545 \n",
      " ChatCompletion(id='chatcmpl-CBYFfeCosk79VvphjcGxlHekhHput', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869307, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2.170250177383423 \n",
      " ChatCompletion(id='chatcmpl-CBYFguw01L73j0yyZHwmRsHrFi3uW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 제가 그들이 관심이 있다는 것을 들었을 때는 마치, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869308, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=62, prompt_tokens=141, total_tokens=203, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.6452639102935791 \n",
      " ChatCompletion(id='chatcmpl-CBYFiQqvzvHfswkUKdcmQFHjbJwcD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869310, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "totals = 0\n",
    "for _ in range(12):\n",
    "    st = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator specializing in [English] → [Korean] translation. Your job is to incrementally translate Korean speech as it comes in.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치...\n",
    "\"\"\"}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        # stream=True,\n",
    "        # stream_options={\"include_usage\": True},\n",
    "    )\n",
    "    \n",
    "    # sent = ''\n",
    "    \n",
    "    # pt = 0\n",
    "    # pt_cached = 0\n",
    "    # ct = 0\n",
    "    \n",
    "    # for chunk in response:\n",
    "    #     if chunk.usage and chunk.usage is not None:\n",
    "    #         if pt == 0:\n",
    "    #             # print(time.time() - st)\n",
    "    #             pt += 1\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "    #             sent += chunk.choices[0].delta.content\n",
    "    \n",
    "    print(time.time() - st, \"\\n\", response)\n",
    "    totals += time.time() - st\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1cbdc-ce39-46a3-a28d-8a88a0361250",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"And there's a dimension of human intelligence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d2562-373b-4019-a01e-ad61f33b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return new[:i] + new[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc034e-31e5-4a3c-88bf-d25ba18649af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,.]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,.]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return old[:i] + new[i:]\n",
    "\n",
    "old = 'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "new = 'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "\n",
    "text_pr(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de166c-2011-47aa-aee3-0c85f7b6b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['So you stop saving in US government.',\n",
    "'So you stop saving in US government bonds.',\n",
    "'So you stop saving in US government bonds and you',\n",
    "'So you stop saving in US government bonds and you start',\n",
    "'So you stop saving in US government bonds and you start saving.',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hard',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest way.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "]\n",
    "\n",
    "tt = ''\n",
    "for t in texts:\n",
    "    tt = text_pr(tt, t)\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7454c-1b1d-4e8d-bfa0-573c5a186bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ['weaf', '222', '333', '444', '555']\n",
    "\n",
    "text = \"\\n\".join([f\"<{x}>\" for x in ss])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8015-d4a2-4a9e-b5a1-ddfa24711fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'de' in {'de': 123}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624cae-cb29-42e6-aba0-266203607a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ceabc-f04c-4b21-beb6-7326693be9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stt.asr import load_asr_backend\n",
    "ASR = load_asr_backend(kind=\"nemo\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdd50f-6344-46d5-8bbe-6001e3de5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "audio, sr = librosa.load(\"./utils/individualAudio.wav\", mono=True, sr=16000)\n",
    "# audio = torch.tensor(audio).to('cuda')\n",
    "pcm_bytes = (np.clip(audio, -1.0, 1.0) * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "st = time.time()\n",
    "ASR.transcribe_pcm(pcm_bytes, sr, 1, language=\"english\")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db786f-37f9-47f1-8d04-a9814c2c68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5693-342d-4d99-9616-6ea8eb22d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
