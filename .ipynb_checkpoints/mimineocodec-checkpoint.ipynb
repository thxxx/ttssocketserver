{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4ec16-6173-4c41-aa8e-5de836211b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from nemo.collections.tts.models import AudioCodecModel\n",
    "\n",
    "model_name = \"nvidia/mel-codec-22khz\"\n",
    "nemo_codec_model = AudioCodecModel.from_pretrained(model_name).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba1611-bca6-4374-be2a-bbb185ab90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import wget\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Utility for displaying signals and metrics\n",
    "def show_signal(signal: np.ndarray, sample_rate: int = 16000, tag: str = 'Signal'):\n",
    "    \"\"\"Show the time-domain signal and its spectrogram.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 2.5))\n",
    "\n",
    "    # show waveform\n",
    "    t = np.arange(0, len(signal)) / sample_rate\n",
    "\n",
    "    ax[0].plot(t, signal)\n",
    "    ax[0].set_xlim(0, t.max())\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlabel('time / s')\n",
    "    ax[0].set_ylabel('amplitude')\n",
    "    ax[0].set_title(tag)\n",
    "\n",
    "    n_fft = 1024\n",
    "    hop_length = 256\n",
    "\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sample_rate, n_fft=n_fft, hop_length=hop_length, ax=ax[1])\n",
    "    ax[1].set_title(tag)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(img, format=\"%+2.f dB\", ax=ax)\n",
    "\n",
    "\n",
    "# Utility for displaying a latent representation\n",
    "def show_latent(latent: np.ndarray, tag: str):\n",
    "    plt.figure(figsize = (16, 3))\n",
    "    img = plt.imshow(latent, aspect='equal')\n",
    "    plt.colorbar(img, ax=plt.gca())\n",
    "    plt.title(tag)\n",
    "    plt.xlabel('Time frame')\n",
    "    plt.ylabel('Latent vector index')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da292f4a-bec8-4b02-9b07-a6204ce1d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import Audio\n",
    "\n",
    "path_to_input_audio = './eo_05.mp3' # path of the input audio\n",
    "path_to_output_audio = './eo_05_recon.mp3' # path of the reconstructed output audio\n",
    "\n",
    "# get discrete tokens from audio\n",
    "audio, _ = librosa.load(path_to_input_audio, sr=nemo_codec_model.sample_rate)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "audio_tensor = torch.from_numpy(audio).unsqueeze(dim=0).to(device)\n",
    "audio_len = torch.tensor([audio_tensor[0].shape[0]]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    st = time.time()\n",
    "    encoded_tokens, encoded_len = nemo_codec_model.encode(audio=audio_tensor, audio_len=audio_len)\n",
    "    print(time.time() - st)\n",
    "    \n",
    "    # Reconstruct audio from tokens\n",
    "    st = time.time()\n",
    "    reconstructed_audio, _ = nemo_codec_model.decode(tokens=encoded_tokens, tokens_len=encoded_len)\n",
    "    print(time.time() - st)\n",
    "\n",
    "# save reconstructed audio\n",
    "output_audio = reconstructed_audio.cpu().numpy().squeeze()\n",
    "sf.write(path_to_output_audio, output_audio, nemo_codec_model.sample_rate)\n",
    "\n",
    "display(Audio(path_to_input_audio))\n",
    "display(Audio(path_to_output_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77576e-313e-4cb3-9dde-3a5124caf7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37308aa-9720-4451-a663-b9ab678a95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape[-1]/nemo_codec_model.sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf1811-217e-4839-a0ad-ba0451189c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio to the encoded representation\n",
    "encoded, encoded_len = nemo_codec_model.encode_audio(audio=audio_tensor, audio_len=audio_len)\n",
    "\n",
    "print('encoded information:')\n",
    "print(f'\\tshape (batch, codebook, time frame) : {encoded.size()}')\n",
    "print(f'\\tdtype                               : {encoded.dtype}')\n",
    "print(f'\\tmin                                 : {encoded.min()}')\n",
    "print(f'\\tmax                                 : {encoded.max()}')\n",
    "\n",
    "\n",
    "# Show the encoded representation\n",
    "show_latent(encoded.detach().squeeze().cpu().numpy(), tag='Encoder output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae27e2-4a32-4bf2-a59c-9c82985a3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder output to tokens\n",
    "tokens = nemo_codec_model.quantize(encoded=encoded, encoded_len=encoded_len)\n",
    "print(tokens.shape)\n",
    "\n",
    "# Tokens back to a continuous vector\n",
    "dequantized = nemo_codec_model.dequantize(tokens=tokens, tokens_len=encoded_len)\n",
    "print(dequantized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522baa3-384f-4bed-8298-4ffddfca82b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69482f-3a2e-416d-8655-8c918ac2995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import MimiModel, AutoFeatureExtractor\n",
    "\n",
    "# load the model + feature extractor (for pre-processing the audio)\n",
    "model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec54b3-099b-4484-a68f-5e06334220e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_input_audio = './eo_05.mp3' # path of the input audio\n",
    "path_to_output_audio = './eo_05_recon.mp3' # path of the reconstructed output audio\n",
    "\n",
    "# get discrete tokens from audio\n",
    "audio, _ = librosa.load(path_to_input_audio, sr=feature_extractor.sampling_rate)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# audio_tensor = torch.from_numpy(audio).unsqueeze(dim=0).to(device)\n",
    "# audio_len = torch.tensor([audio_tensor[0].shape[0]]).to(device)\n",
    "\n",
    "# pre-process the inputs\n",
    "inputs = feature_extractor(raw_audio=audio, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "# explicitly encode then decode the audio inputs\n",
    "encoder_outputs = model.encode(inputs[\"input_values\"].to(device))\n",
    "st = time.time()\n",
    "with torch.inference_mode(), torch.autocast(device_type='cuda', enabled=(device==\"cuda\")):\n",
    "    audio_values = model.decode(encoder_outputs.audio_codes)[0]\n",
    "print(time.time() - st)\n",
    "\n",
    "# or the equivalent with a forward pass\n",
    "audio_values = model(inputs[\"input_values\"].to(device)).audio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eec73d-9b94-4521-ba0b-c834b811fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_to_latent(inputs[\"input_values\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2934e2-bfda-4053-b13b-3b1c1cd997a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
