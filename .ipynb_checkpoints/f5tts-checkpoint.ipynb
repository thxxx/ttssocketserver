{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe791c2-6097-4559-b493-035792f648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from importlib.resources import files\n",
    "\n",
    "import soundfile as sf\n",
    "import tqdm\n",
    "from cached_path import cached_path\n",
    "from hydra.utils import get_class\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from f5_tts.infer.utils_infer import (\n",
    "    infer_process,\n",
    "    load_model,\n",
    "    load_vocoder,\n",
    "    preprocess_ref_audio_text,\n",
    "    remove_silence_for_generated_wav,\n",
    "    save_spectrogram,\n",
    "    transcribe,\n",
    ")\n",
    "from f5_tts.model.utils import seed_everything\n",
    "\n",
    "\n",
    "class F5TTS:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=\"F5TTS_v1_Base\",\n",
    "        ckpt_file=\"\",\n",
    "        vocab_file=\"\",\n",
    "        ode_method=\"euler\",\n",
    "        use_ema=True,\n",
    "        vocoder_local_path=None,\n",
    "        device=None,\n",
    "        hf_cache_dir=None,\n",
    "    ):\n",
    "        model_cfg = OmegaConf.load(str(files(\"f5_tts\").joinpath(f\"configs/{model}.yaml\")))\n",
    "        model_cls = get_class(f\"f5_tts.model.{model_cfg.model.backbone}\")\n",
    "        model_arc = model_cfg.model.arch\n",
    "\n",
    "        self.mel_spec_type = model_cfg.model.mel_spec.mel_spec_type\n",
    "        self.target_sample_rate = model_cfg.model.mel_spec.target_sample_rate\n",
    "\n",
    "        self.ode_method = ode_method\n",
    "        self.use_ema = use_ema\n",
    "\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "        else:\n",
    "            import torch\n",
    "\n",
    "            self.device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"xpu\"\n",
    "                if torch.xpu.is_available()\n",
    "                else \"mps\"\n",
    "                if torch.backends.mps.is_available()\n",
    "                else \"cpu\"\n",
    "            )\n",
    "\n",
    "        # Load models\n",
    "        self.vocoder = load_vocoder(\n",
    "            self.mel_spec_type, vocoder_local_path is not None, vocoder_local_path, self.device, hf_cache_dir\n",
    "        )\n",
    "\n",
    "        repo_name, ckpt_step, ckpt_type = \"F5-TTS\", 1250000, \"safetensors\"\n",
    "\n",
    "        if not ckpt_file:\n",
    "            ckpt_file = str(\n",
    "                cached_path(f\"hf://SWivid/{repo_name}/{model}/model_{ckpt_step}.{ckpt_type}\", cache_dir=hf_cache_dir)\n",
    "            )\n",
    "        \n",
    "        self.ema_model = load_model(\n",
    "            model_cls, model_arc, ckpt_file, self.mel_spec_type, vocab_file, self.ode_method, self.use_ema, self.device\n",
    "        )\n",
    "\n",
    "    def transcribe(self, ref_audio, language=None):\n",
    "        return transcribe(ref_audio, language)\n",
    "\n",
    "    def export_wav(self, wav, file_wave, remove_silence=False):\n",
    "        sf.write(file_wave, wav, self.target_sample_rate)\n",
    "\n",
    "        if remove_silence:\n",
    "            remove_silence_for_generated_wav(file_wave)\n",
    "\n",
    "    def export_spectrogram(self, spec, file_spec):\n",
    "        save_spectrogram(spec, file_spec)\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        ref_file,\n",
    "        ref_text,\n",
    "        gen_text,\n",
    "        show_info=print,\n",
    "        progress=tqdm,\n",
    "        target_rms=0.1,\n",
    "        cross_fade_duration=0.15,\n",
    "        sway_sampling_coef=-1,\n",
    "        cfg_strength=2,\n",
    "        nfe_step=32,\n",
    "        speed=1.0,\n",
    "        fix_duration=None,\n",
    "        remove_silence=False,\n",
    "        file_wave=None,\n",
    "        file_spec=None,\n",
    "        seed=None,\n",
    "    ):\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, sys.maxsize)\n",
    "        seed_everything(seed)\n",
    "        self.seed = seed\n",
    "\n",
    "        ref_file, ref_text = preprocess_ref_audio_text(ref_file, ref_text)\n",
    "\n",
    "        wav, sr, spec = infer_process(\n",
    "            ref_file,\n",
    "            ref_text,\n",
    "            gen_text,\n",
    "            self.ema_model,\n",
    "            self.vocoder,\n",
    "            self.mel_spec_type,\n",
    "            show_info=show_info,\n",
    "            progress=progress,\n",
    "            target_rms=target_rms,\n",
    "            cross_fade_duration=cross_fade_duration,\n",
    "            nfe_step=nfe_step,\n",
    "            cfg_strength=cfg_strength,\n",
    "            sway_sampling_coef=sway_sampling_coef,\n",
    "            speed=speed,\n",
    "            fix_duration=fix_duration,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        if file_wave is not None:\n",
    "            self.export_wav(wav, file_wave, remove_silence)\n",
    "\n",
    "        if file_spec is not None:\n",
    "            self.export_spectrogram(spec, file_spec)\n",
    "\n",
    "        return wav, sr, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3569c-de01-4c66-829e-ac0eed8f88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f5tts = F5TTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e479123-c7f2-4503-ba33-19a16459bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr, spec = f5tts.infer(\n",
    "    ref_file=str(files(\"f5_tts\").joinpath(\"infer/examples/basic/basic_ref_en.wav\")),\n",
    "    ref_text=\"some call me nature, others call me mother nature.\",\n",
    "    gen_text=\"\"\"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall.\"\"\",\n",
    "    file_wave=None,\n",
    "    file_spec=None,\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "print(\"seed :\", f5tts.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962adff-34bb-47c3-80dd-b5b7d8a426c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape[-1]/24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8bf0e-74fc-4f8b-b1d8-1cc8f806e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools import AudioSignal\n",
    "\n",
    "AudioSignal(wav, sample_rate=24000).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ece71-cb4f-4391-8c9c-34d2e59ccbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ef434-53ef-468c-ac09-7db9411d3bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95751d-4783-4e77-bbae-997277c6556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from f5_tts.model.modules import ConvNeXtV2Block\n",
    "\n",
    "conv = ConvNeXtV2Block(256, 256 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6025805-e573-4544-b5dc-d3e78778c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GRN (Global Response Normalization) Layer as a prerequisite for ConvNeXtV2Block\n",
    "# This is a common implementation of GRN.\n",
    "class GRN(nn.Module):\n",
    "    \"\"\" GRN (Global Response Normalization) layer \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Gx = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "# The user-provided ConvNeXtV2Block\n",
    "class ConvNeXtV2Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block of ConvNeXtV2.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        intermediate_dim: int,\n",
    "        dilation: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Calculate padding based on kernel size and dilation to maintain sequence length\n",
    "        padding = (dilation * (7 - 1)) // 2\n",
    "        # Depthwise convolution\n",
    "        self.dwconv = nn.Conv1d(\n",
    "            dim, dim, kernel_size=7, padding=padding, groups=dim, dilation=dilation\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        # Pointwise convolutions implemented as linear layers\n",
    "        self.pwconv1 = nn.Linear(dim, intermediate_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(intermediate_dim)\n",
    "        self.pwconv2 = nn.Linear(intermediate_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        # Transpose for Conv1d: (batch, sequence, dim) -> (batch, dim, sequence)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dwconv(x)\n",
    "        # Transpose back: (batch, dim, sequence) -> (batch, sequence, dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        # Add residual connection\n",
    "        return residual + x\n",
    "\n",
    "# Time Conditioning Block as described in the paper (Appendix A.2.3)\n",
    "class TimeCondBlock(nn.Module):\n",
    "    \"\"\" Conditions the input on a time embedding. \"\"\"\n",
    "    def __init__(self, hidden_dim, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(time_emb_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Project time embedding to match the hidden dimension\n",
    "        time_proj = self.linear(t)\n",
    "        # Add globally to the sequence (unsqueezing for broadcasting)\n",
    "        return x + time_proj.unsqueeze(1)\n",
    "\n",
    "# Text and Reference Conditioning Blocks using Cross-Attention\n",
    "class CrossAttentionCondBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditions the input on text or reference speech using cross-attention.\n",
    "    'q' is the input sequence, 'k' and 'v' are the conditioning variables.\n",
    "    This version uses F.scaled_dot_product_attention for efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        # Linear projections for query, key, value\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, key, value):\n",
    "        batch_size, seq_len_q, _ = x.shape\n",
    "        _, seq_len_kv, _ = key.shape\n",
    "\n",
    "        # 1. Project Q, K, V\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (batch, seq_len, hidden_dim) -> (batch, num_heads, seq_len, head_dim)\n",
    "        q = q.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply scaled dot-product attention\n",
    "        # The output of this function will have the same shape as the query\n",
    "        attn_output = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        # 4. Reshape back and project\n",
    "        # (batch, num_heads, seq_len, head_dim) -> (batch, seq_len, hidden_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, -1)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        # 5. Add residual connection\n",
    "        return x + attn_output\n",
    "\n",
    "# The main VF Estimator model\n",
    "class VFEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Field (VF) Estimator from the Supertonic-TTS paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=144, hidden_dim=256, time_emb_dim=64, text_cond_dim=128, ref_cond_dim=128, num_main_blocks=4):\n",
    "        super().__init__()\n",
    "        # Initial projection layer\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Main repeating blocks\n",
    "        self.main_blocks = nn.ModuleList()\n",
    "        for _ in range(num_main_blocks):\n",
    "            block = nn.Sequential(\n",
    "                # Dilated ConvNeXt blocks\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=2),\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=4),\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=8),\n",
    "                # Standard ConvNeXt blocks\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "                ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "            )\n",
    "            self.main_blocks.append(block)\n",
    "\n",
    "        # Conditioning modules\n",
    "        self.time_cond_blocks = nn.ModuleList([TimeCondBlock(hidden_dim, time_emb_dim) for _ in range(num_main_blocks)])\n",
    "        self.text_cond_blocks = nn.ModuleList([CrossAttentionCondBlock(hidden_dim) for _ in range(num_main_blocks)])\n",
    "        self.ref_cond_blocks = nn.ModuleList([CrossAttentionCondBlock(hidden_dim) for _ in range(num_main_blocks)])\n",
    "\n",
    "        # Final processing blocks\n",
    "        self.final_blocks = nn.Sequential(\n",
    "            ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "            ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "            ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "            ConvNeXtV2Block(hidden_dim, hidden_dim * 4, dilation=1),\n",
    "        )\n",
    "\n",
    "        # Output projection layer\n",
    "        self.output_proj = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "\n",
    "    def forward(self, noisy_latents, time_embedding, text_cond, ref_cond_key, ref_cond_value):\n",
    "        # 1. Initial Projection\n",
    "        x = self.input_proj(noisy_latents)\n",
    "\n",
    "        # 2. Main Blocks with Conditioning\n",
    "        for i, block in enumerate(self.main_blocks):\n",
    "            x = block(x)\n",
    "            x = self.time_cond_blocks[i](x, time_embedding)\n",
    "            x = self.text_cond_blocks[i](x, key=text_cond, value=text_cond)\n",
    "            x = self.ref_cond_blocks[i](x, key=ref_cond_key, value=ref_cond_value)\n",
    "\n",
    "        tt = time.time()\n",
    "        x = self.final_blocks(x)\n",
    "        print(f\"[{time.time() - tt}] Final blocks\")\n",
    "        output = self.output_proj(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df32b9-1e68-4b36-bbdf-f1daf98c16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dimensions are based on the paper's appendix\n",
    "vf_estimator = VFEstimator(\n",
    "    input_dim=144,       # Dimension of compressed latents\n",
    "    hidden_dim=256,      # Hidden dimension inside the estimator\n",
    "    time_emb_dim=64,     # Dimension of the time embedding\n",
    "    text_cond_dim=256,   # Dimension of the text encoder output\n",
    "    ref_cond_dim=256,    # Dimension of the reference encoder output\n",
    "    num_main_blocks=4    # Number of repeating main blocks\n",
    ").to('cuda')\n",
    "\n",
    "# --- Dummy Input Creation ---\n",
    "batch_size = 1\n",
    "sequence_length = 200 # Example sequence length for the latents\n",
    "text_seq_len = 50     # Example sequence length for text condition\n",
    "ref_seq_len = 50      # Example sequence length for reference condition\n",
    "\n",
    "# Noisy latents (output from forward process)\n",
    "# Shape: (batch_size, sequence_length, input_dim)\n",
    "dummy_noisy_latents = torch.randn(batch_size, sequence_length, 144)\n",
    "\n",
    "# Time embedding for the current diffusion step\n",
    "# Shape: (batch_size, time_emb_dim)\n",
    "dummy_time_embedding = torch.randn(batch_size, 64)\n",
    "\n",
    "# Text conditioning variable (output from Text Encoder)\n",
    "# Shape: (batch_size, text_seq_len, text_cond_dim)\n",
    "dummy_text_cond = torch.randn(batch_size, text_seq_len, 256)\n",
    "\n",
    "# Reference conditioning variables (output from Reference Encoder)\n",
    "# Shape: (batch_size, ref_seq_len, ref_cond_dim)\n",
    "dummy_ref_key = torch.randn(batch_size, ref_seq_len, 256)\n",
    "dummy_ref_value = torch.randn(batch_size, ref_seq_len, 256)\n",
    "\n",
    "\n",
    "# --- Model Forward Pass ---\n",
    "print(\"--- Running VF Estimator ---\")\n",
    "print(f\"Input noisy_latents shape: {dummy_noisy_latents.shape}\")\n",
    "\n",
    "# The model might be large, so let's put it in eval mode for inference\n",
    "vf_estimator.eval()\n",
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    st = time.time()\n",
    "    output_vector_field = vf_estimator(\n",
    "        dummy_noisy_latents.to('cuda'),\n",
    "        dummy_time_embedding.to('cuda'),\n",
    "        dummy_text_cond.to('cuda'),\n",
    "        dummy_ref_key.to('cuda'),\n",
    "        dummy_ref_value.to('cuda')\n",
    "    )\n",
    "    print(time.time() - st)\n",
    "\n",
    "print(f\"Output vector_field shape: {output_vector_field.shape}\")\n",
    "print(\"Model ran successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501fb4b-fa66-4607-97f0-da208ed678b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vf_estimator.py\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# Speed knobs (optional)\n",
    "# ----------------------------\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------------------------\n",
    "# GRN (for ConvNeXt V2 block)\n",
    "# ----------------------------\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, N, C)\n",
    "        # Global Response Normalization (ConvNeXt V2)\n",
    "        Gx = torch.norm(x, p=2, dim=-1, keepdim=True)                 # (B, N, 1)\n",
    "        Nx = Gx / (Gx.mean(dim=-2, keepdim=True) + self.eps)          # (B, N, 1)\n",
    "        return self.gamma * (x * Nx) + self.beta + x\n",
    "\n",
    "# ----------------------------\n",
    "# Your ConvNeXtV2Block (as given)\n",
    "# ----------------------------\n",
    "class ConvNeXtV2Block(nn.Module):\n",
    "    def __init__(self, dim: int, intermediate_dim: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        padding = (dilation * (7 - 1)) // 2\n",
    "        self.dwconv = nn.Conv1d(\n",
    "            dim, dim, kernel_size=7, padding=padding, groups=dim, dilation=dilation\n",
    "        )  # depthwise conv\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, intermediate_dim)  # pointwise/1x1 convs\n",
    "        self.act = nn.GELU()\n",
    "        self.grn = GRN(intermediate_dim)\n",
    "        self.pwconv2 = nn.Linear(intermediate_dim, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, N, D)\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)             # (B, D, N)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.transpose(1, 2)             # (B, N, D)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return residual + x\n",
    "\n",
    "# ----------------------------\n",
    "# Time embedding (Grad-TTS style)\n",
    "# ----------------------------\n",
    "def sinusoidal_t_embed(t: torch.Tensor, dim: int = 64) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    t: (B,) in [0,1], returns (B, dim) sinusoidal time embedding.\n",
    "    \"\"\"\n",
    "    device = t.device\n",
    "    half = dim // 2\n",
    "    # 2*pi for better coverage; consistent with many diffusion impls\n",
    "    freqs = torch.exp(\n",
    "        torch.linspace(0, math.log(10000), half, device=device)\n",
    "        * (-1)\n",
    "    )\n",
    "    # shape: (B, half)\n",
    "    args = 2.0 * math.pi * t[:, None] * freqs[None, :]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb  # (B, dim)\n",
    "\n",
    "# ----------------------------\n",
    "# Cross-Attention Block (uses SDPA)\n",
    "# ----------------------------\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim_q: int, dim_kv: int, num_heads: int = 4, head_dim: int = 64, attn_drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.inner_dim = num_heads * head_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(dim_q, self.inner_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(dim_kv, self.inner_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(dim_kv, self.inner_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(self.inner_dim, dim_q, bias=True)\n",
    "        self.attn_drop = attn_drop\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_q: torch.Tensor,      # (B, Tq, Dq)\n",
    "        x_kv: torch.Tensor,     # (B, Tk, Dkv)\n",
    "        attn_mask: Optional[torch.Tensor] = None  # (B, Tq, Tk) or None\n",
    "    ) -> torch.Tensor:\n",
    "        B, Tq, _ = x_q.shape\n",
    "        _, Tk, _ = x_kv.shape\n",
    "\n",
    "        q = self.q_proj(x_q)  # (B, Tq, inner)\n",
    "        k = self.k_proj(x_kv) # (B, Tk, inner)\n",
    "        v = self.v_proj(x_kv) # (B, Tk, inner)\n",
    "\n",
    "        # reshape to (B, heads, T, head_dim)\n",
    "        q = q.view(B, Tq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, Tk, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, Tk, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product attention (uses FlashAttention kernels when available)\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=self.attn_drop if self.training else 0.0, is_causal=False\n",
    "        )  # (B, heads, Tq, head_dim)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Tq, self.inner_dim)\n",
    "        return self.out_proj(out)  # (B, Tq, Dq)\n",
    "\n",
    "# ----------------------------\n",
    "# VF Estimator\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class VFConfig:\n",
    "    d_in: int = 144        # compressed latent dim (C*Kc)\n",
    "    d_model: int = 256     # hidden channels inside VF\n",
    "    d_time: int = 64       # time embedding dim (before projection)\n",
    "    n_repeats: int = 4     # Nm in the paper\n",
    "    n_post: int = 4        # post ConvNeXt blocks count\n",
    "    inter_dim: int = 1024  # ConvNeXt MLP hidden\n",
    "    kernel_size: int = 5\n",
    "    num_heads: int = 4\n",
    "    head_dim: int = 64\n",
    "\n",
    "class VFEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the VF Estimator (Fig. 4(c) & Appx A.2.3) of Supertonic-TTS.\n",
    "    - Input: noisy compressed latents z_t (B, T, 144)\n",
    "    - Conditions: time t (B,), text_kv (B, Nt, Dt), ref_kv (B, Nr, Dr)\n",
    "    - Output: vector field v(z_t, text, ref, t) with shape (B, T, 144)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: VFConfig = VFConfig()):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # input/output projections\n",
    "        self.proj_in  = nn.Linear(cfg.d_in, cfg.d_model)\n",
    "        self.proj_out = nn.Linear(cfg.d_model, cfg.d_in)\n",
    "\n",
    "        # time conditioning: 64-d sinusoidal -> project to d_model, then global add\n",
    "        self.time_proj = nn.Linear(cfg.d_time, cfg.d_model)\n",
    "\n",
    "        # text & ref cross-attention (reused many times)\n",
    "        self.text_attn = CrossAttention(cfg.d_model, dim_kv=128, num_heads=cfg.num_heads, head_dim=cfg.head_dim)\n",
    "        self.ref_attn  = CrossAttention(cfg.d_model, dim_kv=128, num_heads=cfg.num_heads, head_dim=cfg.head_dim)\n",
    "\n",
    "        # a helper to build blocks\n",
    "        def convnext_block(dilation: int = 1):\n",
    "            return ConvNeXtV2Block(dim=cfg.d_model, intermediate_dim=cfg.inter_dim, dilation=dilation)\n",
    "\n",
    "        # main repeated stages\n",
    "        self.stages = nn.ModuleList()\n",
    "        for _ in range(cfg.n_repeats):\n",
    "            stage = nn.ModuleDict(\n",
    "                dict(\n",
    "                    dilated_1=convnext_block(dilation=1),\n",
    "                    dilated_2=convnext_block(dilation=2),\n",
    "                    dilated_3=convnext_block(dilation=4),\n",
    "                    dilated_4=convnext_block(dilation=8),\n",
    "                    # after time add, two standard ConvNeXt blocks around cross-attn\n",
    "                    pre_text = convnext_block(dilation=1),\n",
    "                    post_text= convnext_block(dilation=1),\n",
    "                )\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "\n",
    "        # post blocks\n",
    "        self.post_blocks = nn.Sequential(*[convnext_block(dilation=1) for _ in range(cfg.n_post)])\n",
    "\n",
    "        # lightweight norms on the path\n",
    "        self.pre_text_ln = nn.LayerNorm(cfg.d_model)\n",
    "        self.post_text_ln = nn.LayerNorm(cfg.d_model)\n",
    "        self.pre_ref_ln = nn.LayerNorm(cfg.d_model)\n",
    "        self.post_ref_ln = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        zt: torch.Tensor,               # (B, T, 144)\n",
    "        t: torch.Tensor,                # (B,)\n",
    "        text_kv: torch.Tensor,          # (B, Nt, 128)\n",
    "        ref_kv: torch.Tensor,           # (B, Nr, 128)\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.proj_in(zt)  # (B, T, d_model)\n",
    "\n",
    "        # precompute time embedding once\n",
    "        t_emb = sinusoidal_t_embed(t, dim=self.cfg.d_time)\n",
    "        t_add = self.time_proj(t_emb)  # (B, d_model)\n",
    "\n",
    "        for stage in self.stages:\n",
    "            # 4 dilated ConvNeXt blocks\n",
    "            x = stage[\"dilated_1\"](x)\n",
    "            x = stage[\"dilated_2\"](x)\n",
    "            x = stage[\"dilated_3\"](x)\n",
    "            x = stage[\"dilated_4\"](x)\n",
    "\n",
    "            # time conditioning: global add (broadcast over time)\n",
    "            x = x + t_add[:, None, :]\n",
    "\n",
    "            # ConvNeXt -> Text cross-attn -> ConvNeXt\n",
    "            x = stage[\"pre_text\"](x)\n",
    "            x = self.pre_text_ln(x)\n",
    "            x = x + self.text_attn(x, text_kv)  # residual\n",
    "            x = stage[\"post_text\"](x)\n",
    "\n",
    "            # LayerNorm -> Ref cross-attn (residual)\n",
    "            x = self.pre_ref_ln(x)\n",
    "            x = x + self.ref_attn(x, ref_kv)\n",
    "            x = self.post_ref_ln(x)\n",
    "\n",
    "        # tail ConvNeXt stack + projection to 144-dim\n",
    "        x = self.post_blocks(x)\n",
    "        out = self.proj_out(x)  # (B, T, 144)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d808a-3854-48c0-b9a3-ae32c41c20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Quick demo (random inputs)\n",
    "# ----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "B, T = 2, 860     # batch, time steps of compressed latents\n",
    "Nt, Nr = 256, 50  # text tokens, reference tokens\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "zt = torch.randn(B, T, 144, device=device, dtype=dtype)\n",
    "t  = torch.rand(B, device=device, dtype=torch.float32)  # keep time in fp32 for embedding stability\n",
    "text_kv = torch.randn(B, Nt, 128, device=device, dtype=dtype)\n",
    "ref_kv  = torch.randn(B, Nr, 128, device=device, dtype=dtype)\n",
    "\n",
    "model = VFEstimator().to(device=device, dtype=dtype)\n",
    "model.eval()\n",
    "\n",
    "# Optional: compile for speed (PyTorch 2.0+)\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Inference\n",
    "with torch.inference_mode(), torch.autocast(device_type=device if device == \"cuda\" else \"cpu\", enabled=(device==\"cuda\")):\n",
    "    y = model(zt, t, text_kv, ref_kv)\n",
    "\n",
    "print(\"Output shape:\", y.shape)  # (B, T, 144)\n",
    "\n",
    "# Small timing test\n",
    "iters = 10\n",
    "torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "import time\n",
    "st = time.time()\n",
    "for _ in range(iters):\n",
    "    with torch.inference_mode(), torch.autocast(device_type=device if device==\"cuda\" else \"cpu\", enabled=(device==\"cuda\")):\n",
    "        _ = model(zt, t, text_kv, ref_kv)\n",
    "torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "print(f\"{iters} iters avg latency: {(time.time()-st)*1000:.2f} ms\")\n",
    "\n",
    "# 길이 200 -> 120, latency 60ms -> 64?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526c55d-4e31-43f8-a7fb-1defc23b1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 파라미터 수 계산\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5eef6e-ce37-4238-8cf0-3b75a608e4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
