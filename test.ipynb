{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41583911-fbbb-4604-a73f-cef3df232bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 06:56:57 [__init__.py:256] Automatically detected platform cuda.\n",
      "INFO 08-31 06:56:58 [config.py:2595] Downcasting torch.float32 to torch.float16.\n",
      "INFO 08-31 06:57:05 [config.py:583] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-31 06:57:05 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-31 06:57:05 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-31 06:57:07 [core.py:53] Initializing a V1 LLM engine (v0.8.0) with config: model='trillionlabs/Tri-1.8B-Translation', speculative_config=None, tokenizer='trillionlabs/Tri-1.8B-Translation', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=trillionlabs/Tri-1.8B-Translation, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 08-31 06:57:08 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78fd60781540>\n",
      "INFO 08-31 06:57:08 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-31 06:57:08 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-31 06:57:08 [gpu_model_runner.py:1128] Starting to load model trillionlabs/Tri-1.8B-Translation...\n",
      "WARNING 08-31 06:57:08 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-31 06:57:09 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7384ed77244f3b866e4262571f6bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 06:57:15 [loader.py:429] Loading weights took 6.27 seconds\n",
      "INFO 08-31 06:57:15 [gpu_model_runner.py:1140] Model loading took 3.3748 GB and 6.688703 seconds\n",
      "INFO 08-31 06:57:16 [kv_cache_utils.py:537] GPU KV cache size: 47,408 tokens\n",
      "INFO 08-31 06:57:16 [kv_cache_utils.py:540] Maximum concurrency for 16,384 tokens per request: 2.89x\n",
      "INFO 08-31 06:57:16 [core.py:138] init engine (profile, create kv cache, warmup model) took 0.88 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"trillionlabs/Tri-1.8B-Translation\",\n",
    "    dtype=\"float16\",               # GPU라면 권장\n",
    "    tensor_parallel_size=1,        # 단일 GPU 보장\n",
    "    enforce_eager=True,            # CUDA graph 캡쳐 이슈 회피\n",
    "    gpu_memory_utilization=0.5,   # 메모리 여유 확보\n",
    "    disable_log_stats=True\n",
    ")\n",
    "sp = SamplingParams(temperature=0.3, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b618c23d-68ef-4d74-80ba-ea2bd31c61f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 135.14 toks/s, output: 107.78 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'어, 그게 어땠나요? 몇 년도였나요? 우리에게 그 자리를 줘요. 네, 알렉스. 그러니까 우리가 오늘 이야기하고 싶은 대부분의 것은 지금 Scale이 하고 있는 일인데, 그 이유는 Scale이 YC에서 시작했을 때부터 지금까지 계속 진행되어 온 일이기 때문입니다.\\n\\n그래서 흥미롭고 정말 대단합니다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"ko\"\n",
    "text = \"\"\"Uh what was that like? What year was it? Put us in the spot. Yeah, Alex. I mean, most of what\n",
    "we want to talk about today is like what Scale is doing now because like the the current stuff is like so so awesome and\n",
    "so interesting since Scale got started at YC.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate into {target}\\n\n",
    "{text}<{target}>\n",
    "\"\"\"\n",
    "out = llm.chat([{\"role\": \"user\", \"content\": prompt}], sampling_params=sp)\n",
    "out[0].outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004363e-f612-4b01-b993-37aa3a17c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning actually. Uh what was that like? What year was it? Put us in the spot. Yeah, Alex. I mean, most of what\n",
    "\n",
    "we want to talk about today is like what Scale is doing now because like the the current stuff is like so so awesome and\n",
    "\n",
    "so interesting since Scale got started at YC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8b09e-7a71-4095-8895-a74c88ed9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"ByteDance-Seed/Seed-X-PPO-7B\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    max_num_seqs=512,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=True, \n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa1425-7cf4-4207-86f8-329facfb5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"Translate the following English sentence into Chinese:\\nMay the force be with you <zh>\",\n",
    "    \"Translate the following English sentence into Chinese and explain it in detail:\\nMay the force be with you <zh>\" \n",
    "]\n",
    "\n",
    "# Beam Search (We recommend using beam search decoding)\n",
    "decoding_params = BeamSearchParams(beam_width=4, \n",
    "                                   max_tokens=512)\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "results = model.generate(messages, decoding_params)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de11b2-4c90-4f5d-a1b3-0acfffb7d1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27eeba-f2b0-4041-8502-74c81b321c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd79cea9-f208-4610-a4ec-ceb25d84c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8045036792755127 \n",
      " ChatCompletion(id='chatcmpl-CAWHLA2D60E91rwHgdIHGi5gvKvkW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 앞으로 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실상 완전 새롭고 미개척된 분야예요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623395, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=42, prompt_tokens=73, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.8031246662139893 \n",
      " ChatCompletion(id='chatcmpl-CAWHM47b747oVig3ipyGCbxZhNsDU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='음, 앞으로 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같지만, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623396, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_37c45ea698', usage=CompletionUsage(completion_tokens=39, prompt_tokens=73, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.53181791305542 \n",
      " ChatCompletion(id='chatcmpl-CAWHNWK1gWk28HdDRIg0Ptf9fPtLv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='음, 아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실 거의 완전 초기 단계라고 할 수 있죠.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623397, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_37c45ea698', usage=CompletionUsage(completion_tokens=40, prompt_tokens=73, total_tokens=113, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.9622204303741455 \n",
      " ChatCompletion(id='chatcmpl-CAWHOlJHEParGgUGowkIY950TC5HP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='음, 앞으로 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623398, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_37c45ea698', usage=CompletionUsage(completion_tokens=39, prompt_tokens=73, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.0379869937896729 \n",
      " ChatCompletion(id='chatcmpl-CAWHP9PjfKbvQhAvLRqbnBtF4Rlln', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같지만, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623399, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=38, prompt_tokens=73, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.7259314060211182 \n",
      " ChatCompletion(id='chatcmpl-CAWHQV3Epf0WDGf6DpkJ10PUhlRdy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 앞으로 5년에서 10년 사이에 꽤 큰 경쟁자가 될 것 같지만, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623400, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=36, prompt_tokens=73, total_tokens=109, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.9134738445281982 \n",
      " ChatCompletion(id='chatcmpl-CAWHRbz2QtsACIIMjpe5Jqwpa7I22', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623401, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=38, prompt_tokens=73, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.8667478561401367 \n",
      " ChatCompletion(id='chatcmpl-CAWHSNLbNGfwpk4FDxHjPc2918qz0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623402, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_4fce0778af', usage=CompletionUsage(completion_tokens=38, prompt_tokens=73, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.033531665802002 \n",
      " ChatCompletion(id='chatcmpl-CAWHTdNnCezTXzqhcRPTgUiUkS5TZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='음, 아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금으로서는 솔직히 완전 초기 단계라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623403, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_37c45ea698', usage=CompletionUsage(completion_tokens=42, prompt_tokens=73, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.1629881858825684 \n",
      " ChatCompletion(id='chatcmpl-CAWHUm1LPN7UlSHrXQDa3FkGLj55Y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같지만, 지금은 사실상 완전히 새로운 시장이라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623404, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=38, prompt_tokens=73, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.406714916229248 \n",
      " ChatCompletion(id='chatcmpl-CAWHV8UhR7yLK5kmZXlpYK3mYSYJ0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='음, 앞으로 5년에서 10년 후쯤에는 꽤 큰 경쟁자들이 될 것 같은데, 지금은 사실 거의 완전한 초기 단계라고 할 수 있죠.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623405, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_6d7dcc9a98', usage=CompletionUsage(completion_tokens=41, prompt_tokens=73, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.8185634613037109 \n",
      " ChatCompletion(id='chatcmpl-CAWHWF9FIjk4eMyYf5WxtBPLneF4M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='아마 5년에서 10년 후쯤에는 꽤 큰 경쟁자가 될 것 같은데, 지금은 사실상 완전히 새로운 분야라고 할 수 있어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756623406, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=38, prompt_tokens=73, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "12.070006132125854\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "totals = 0\n",
    "for _ in range(12):\n",
    "    st = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator specializing in [English] → [Korean] translation. Your job is to incrementally translate Korean speech as it comes in.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    I guess you will end up being pretty big competitors in another 5 10 years but for now like it's basically so green field honestly.\n",
    "    \"\"\"}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        # stream=True,\n",
    "        # stream_options={\"include_usage\": True},\n",
    "    )\n",
    "    \n",
    "    # sent = ''\n",
    "    \n",
    "    # pt = 0\n",
    "    # pt_cached = 0\n",
    "    # ct = 0\n",
    "    \n",
    "    # for chunk in response:\n",
    "    #     if chunk.usage and chunk.usage is not None:\n",
    "    #         if pt == 0:\n",
    "    #             # print(time.time() - st)\n",
    "    #             pt += 1\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "    #             sent += chunk.choices[0].delta.content\n",
    "    \n",
    "    print(time.time() - st, \"\\n\", response)\n",
    "    totals += time.time() - st\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b1cbdc-ce39-46a3-a28d-8a88a0361250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"And there's a dimension of human intelligence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc034e-31e5-4a3c-88bf-d25ba18649af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
