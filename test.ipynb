{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583911-fbbb-4604-a73f-cef3df232bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"trillionlabs/Tri-1.8B-Translation\",\n",
    "    dtype=\"float16\",               # GPU라면 권장\n",
    "    tensor_parallel_size=1,        # 단일 GPU 보장\n",
    "    enforce_eager=True,            # CUDA graph 캡쳐 이슈 회피\n",
    "    gpu_memory_utilization=0.5,   # 메모리 여유 확보\n",
    "    disable_log_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618c23d-68ef-4d74-80ba-ea2bd31c61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SamplingParams(temperature=0.1, max_tokens=512)\n",
    "\n",
    "target = \"ko\"\n",
    "text = \"\"\"There's so much to do on a day like this, lots of things to do, but maybe later when you're having dinner.\"\"\"\n",
    "\n",
    "prompt_old = f\"\"\"\n",
    "Translate into ko\\n\n",
    "{text}<ko>\n",
    "이런 날에는 할 일이 너무 많고 할 일도 많지만, 나중에 저녁을 먹을 때쯤이면.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate into ko\\n\n",
    "There will be a chance to reflect.<ko>\n",
    "\"\"\"\n",
    "out = llm.chat([{\"role\": \"user\", \"content\": prompt}], sampling_params=sp)\n",
    "out[0].outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004363e-f612-4b01-b993-37aa3a17c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a137b4-10f7-499f-b0ba-084fb76867d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8b09e-7a71-4095-8895-a74c88ed9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    max_num_seqs=512,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=True, \n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa1425-7cf4-4207-86f8-329facfb5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.\n",
    "In my team in Antwerp there were some players who were also in England and when I talked to them about Manchester United you could directly see their face change.\n",
    "<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치 어린 시절의 꿈 같았어요.\"\"\",\n",
    "]\n",
    "\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0.1,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "results = model.generate(messages, decoding_params)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de11b2-4c90-4f5d-a1b3-0acfffb7d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "In my team in Antwerp there were some players who were also in England and when I talked to them about Manchester United you could directly see their face change.\n",
    "<ko>\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0.1,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "results = model.generate(messages, decoding_params)\n",
    "print(time.time() - st)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27eeba-f2b0-4041-8502-74c81b321c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79cea9-f208-4610-a4ec-ceb25d84c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "totals = 0\n",
    "for _ in range(12):\n",
    "    st = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator specializing in [English] → [Korean] translation. Your job is to incrementally translate Korean speech as it comes in.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치...\n",
    "\"\"\"}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        # stream=True,\n",
    "        # stream_options={\"include_usage\": True},\n",
    "    )\n",
    "    \n",
    "    # sent = ''\n",
    "    \n",
    "    # pt = 0\n",
    "    # pt_cached = 0\n",
    "    # ct = 0\n",
    "    \n",
    "    # for chunk in response:\n",
    "    #     if chunk.usage and chunk.usage is not None:\n",
    "    #         if pt == 0:\n",
    "    #             # print(time.time() - st)\n",
    "    #             pt += 1\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "    #             sent += chunk.choices[0].delta.content\n",
    "    \n",
    "    print(time.time() - st, \"\\n\", response)\n",
    "    totals += time.time() - st\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1cbdc-ce39-46a3-a28d-8a88a0361250",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"And there's a dimension of human intelligence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d2562-373b-4019-a01e-ad61f33b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return new[:i] + new[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc034e-31e5-4a3c-88bf-d25ba18649af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,.]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,.]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return old[:i] + new[i:]\n",
    "\n",
    "old = 'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "new = 'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "\n",
    "text_pr(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de166c-2011-47aa-aee3-0c85f7b6b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['So you stop saving in US government.',\n",
    "'So you stop saving in US government bonds.',\n",
    "'So you stop saving in US government bonds and you',\n",
    "'So you stop saving in US government bonds and you start',\n",
    "'So you stop saving in US government bonds and you start saving.',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hard',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest way.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "]\n",
    "\n",
    "tt = ''\n",
    "for t in texts:\n",
    "    tt = text_pr(tt, t)\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7454c-1b1d-4e8d-bfa0-573c5a186bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ['weaf', '222', '333', '444', '555']\n",
    "\n",
    "text = \"\\n\".join([f\"<{x}>\" for x in ss])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8015-d4a2-4a9e-b5a1-ddfa24711fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'de' in {'de': 123}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624cae-cb29-42e6-aba0-266203607a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ceabc-f04c-4b21-beb6-7326693be9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stt.asr import load_asr_backend\n",
    "ASR = load_asr_backend(kind=\"nemo\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdd50f-6344-46d5-8bbe-6001e3de5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "audio, sr = librosa.load(\"./utils/individualAudio.wav\", mono=True, sr=16000)\n",
    "# audio = torch.tensor(audio).to('cuda')\n",
    "pcm_bytes = (np.clip(audio, -1.0, 1.0) * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "st = time.time()\n",
    "ASR.transcribe_pcm(pcm_bytes, sr, 1, language=\"english\")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db786f-37f9-47f1-8d04-a9814c2c68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5693-342d-4d99-9616-6ea8eb22d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd469093-36d9-45cd-a320-0cde0a291d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.openai_test import translate_simple\n",
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "def translate_simple(prevScripts:str, current_scripted_sentence:str):\n",
    "    hist = \"\\n\".join([f\" me:{x},\" for x in prevScripts])\n",
    "\n",
    "    st = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator specializing in [Korean] → [English] translation.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "지금 계속 한글로 말하는걸 영어로 번역하고 있어.\n",
    "<previous utterances>는 현재 문장 이전에 이야기하던 문장이야. 번역을 위한 맥락 파악에 사용할 수 있어.\n",
    "<speaking english>은 번역해야하는 현재 발화야.\n",
    "\n",
    "말을 하는걸 script로 만든 input이기 때문에, 발음 문제로 인해서 텍스트가 잘못 들어왔을 수 있어. 그걸 감안해서 번역해줘.\n",
    "\n",
    "출력 english를 일반 글 문장보다는 실제로 사람이 말하는 것 같은 구어체로 적어줘. 예를 들어, Oh, Ah, uhm..을 쓰거나 아님 같은 단어를 두번 쓰거나 이런 것들 있잖아?\n",
    "Translate into casual spoken English. 근데 너무 심하게 하진 말고, 없는 말을 만들거나 들어온 input을 왜곡하면 안돼.\n",
    "\n",
    "-- INPUT --\n",
    "<previous utterances>{hist}\n",
    "<speaking korean> : {current_scripted_sentence}\n",
    "<english> : \n",
    "\"\"\"}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        stream=True,\n",
    "    )\n",
    "    sent = ''\n",
    "    for chunk in response:\n",
    "        if chunk.usage and chunk.usage is not None:\n",
    "            u = chunk.usage;\n",
    "        else:\n",
    "            if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "                print(time.time() - st, \"-\", chunk.choices[0].delta.content)\n",
    "                sent += chunk.choices[0].delta.content\n",
    "            if chunk.choices[0].finish_reason is not None:\n",
    "                print(\"END RETURN!\", time.time() - st)\n",
    "                return sent\n",
    "\n",
    "    return sent\n",
    "\n",
    "def tt(token):\n",
    "    pass\n",
    "\n",
    "sts = time.time()\n",
    "result = translate_simple(\"\", \"아 아 그건 좀 아닌 것 같은데.. 오늘은 뭐 먹을까?ㅋㅋ 맛난거 먹자\")\n",
    "print(result, time.time() - sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3159b5e-d4e6-4fea-891e-c0cfdfdd78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "ctx = load_infer_context({\n",
    "    \"model_name\": \"zipvoice\",\n",
    "    \"model_dir\": None,\n",
    "    \"checkpoint_name\": \"model.pt\",\n",
    "    \"vocoder_path\": None,\n",
    "    \"tokenizer\": \"emilia\",\n",
    "    \"lang\": \"en-us\",\n",
    "    \"num_step\": 32,\n",
    "    \"guidance_scale\": None,\n",
    "    \"feat_scale\": 0.1,\n",
    "    \"speed\": 0.9,\n",
    "    \"t_shift\": 0.5,\n",
    "    \"target_rms\": 0.1,\n",
    "})\n",
    "\n",
    "wav, info = generate_sentence(\n",
    "    prompt_text='Ahh you flipped on me, Oh, that smooth. Honestly, Pretty chill, just existing, you know.',\n",
    "    prompt_wav_path='./denoised.wav',\n",
    "    text=result,\n",
    "    ctx=ctx,\n",
    ")\n",
    "\n",
    "display(Audio(wav, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb62c0-8edc-4bbc-9c4a-ea68ce2f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "audio = AudioSegment.from_file(\"./tts/voice.wav\")\n",
    "display(Audio(\"./tts/voice.wav\"))\n",
    "# 앞 0.1초 (100ms) 추출\n",
    "first_100ms = audio[:800]  # 밀리초 단위\n",
    "\n",
    "# 새로운 파일로 저장\n",
    "first_100ms.export(\"output.wav\", format=\"wav\")\n",
    "display(Audio(\"output.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846f6c4-e353-4590-b068-fe2bf1933de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "audio = AudioSegment.from_file(\"./sam.m4a\")\n",
    "first_100ms = audio[:-500]  # 밀리초 단위\n",
    "first_100ms.export(\"output.wav\", format=\"wav\")\n",
    "display(Audio(\"output.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ccde1-e3d4-49b8-a873-a5534653ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipvoice.tokenizer.tokenizer import EmiliaTokenizer\n",
    "\n",
    "tokenizer = EmiliaTokenizer(token_file=\"/workspace/ttssocketserver/tts/tokens.txt\")\n",
    "\n",
    "print(tokenizer.texts_to_tokens([\"안녕하세요, what's happening? 霍...啦啦啦超过\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841389f1-11ea-4ce8-ab5c-97512384db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import noisereduce as nr\n",
    "import soundfile as sf\n",
    "\n",
    "y, sr = librosa.load(\"./output.wav\", sr=None)\n",
    "\n",
    "noise_clip = y[:int(sr*0.3)]\n",
    "\n",
    "reduced_audio = nr.reduce_noise(y=y, sr=sr, y_noise=noise_clip)\n",
    "\n",
    "sf.write(\"denoised.wav\", reduced_audio, sr)\n",
    "\n",
    "display(Audio(reduced_audio, rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa6918-4419-49f4-9569-d71392a4369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.openai_test import translate_simple\n",
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "def tes(tk):\n",
    "    print(time.time() - st, tk)\n",
    "\n",
    "res = translate_simple(\"\", \"안녕하세요 밥이나 잡수시죠?\", \"\", tes)\n",
    "print(time.time() - st)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dc3817-8f51-4d8a-b2bf-b5e0cddbadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216360,) -0.49958992 0.63853526\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "y, sr = librosa.load(\"./output.wav\", sr=24000)\n",
    "print(y.shape, y.min(), y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5182595c-0a54-4422-b145-60746cc8e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7432799 0.95\n"
     ]
    }
   ],
   "source": [
    "from librosa.util import normalize\n",
    "\n",
    "yy = normalize(y) * 0.95\n",
    "print(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a25487-476f-44ca-a090-158513dfc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "\n",
    "filler_audios_path = [\"./utils/hmhm.wav\", \"./utils/uhuh.wav\", \"./utils/ohoh.wav\", \"./utils/uhmuhm.wav\"]\n",
    "filler_audios = []\n",
    "for p in filler_audios_path:\n",
    "    audiod, sr = librosa.load(p, sr=24000, mono=True)\n",
    "    audiod = torch.tensor(audiod)\n",
    "    filler_audios.append(audiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89c5090-dec9-43bf-92de-3f56c5cfc6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28800])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45bd3a-f51c-46d1-b6c5-b56b204b1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import queue\n",
    "\n",
    "aa = queue.Queue()\n",
    "aa.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517e674-f532-4e0e-8823-e6fea9cc5a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a11683-a25a-4824-b230-a20d5d44aa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c456e596a6ab448e96a77142ffd61c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    }
   ],
   "source": [
    "from chatterbox_infer.mtl_tts import ChatterboxMultilingualTTS\n",
    "tts_model = ChatterboxMultilingualTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6461bb7e-1784-4171-a071-2689f4a36d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m evt \u001b[38;5;129;01min\u001b[39;00m tts_model.generate_stream(\n\u001b[32m      2\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mHello every one!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m         language_id=\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         audio_prompt_path=\u001b[33m\"\u001b[39m\u001b[33m/workspace/ttssocketserver/output.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     ):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m evt.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mchunk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      7\u001b[39m             wav: torch.Tensor = evt[\u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ttssocketserver/chatterbox_infer/mtl_tts.py:364\u001b[39m, in \u001b[36mChatterboxMultilingualTTS.generate_stream\u001b[39m\u001b[34m(self, text, language_id, repetition_penalty, min_p, top_p, audio_prompt_path, exaggeration, cfg_weight, temperature)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m    354\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.t3.inference_streaming(\n\u001b[32m    355\u001b[39m         t3_cond=\u001b[38;5;28mself\u001b[39m.conds.t3,\n\u001b[32m    356\u001b[39m         text_tokens=text_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m         top_p=top_p,\n\u001b[32m    363\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43maudio_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtoken_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ttssocketserver/chatterbox_infer/models/t3/t3.py:507\u001b[39m, in \u001b[36mT3.inference_streaming\u001b[39m\u001b[34m(self, t3_cond, text_tokens, initial_speech_tokens, prepend_prompt_speech_tokens, num_return_sequences, max_new_tokens, stop_on_eos, do_sample, temperature, top_p, min_p, length_penalty, repetition_penalty, cfg_weight)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# Pass the last generated token for repetition tracking\u001b[39;00m\n\u001b[32m    506\u001b[39m     last_token = generated_ids[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m].item() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generated_ids[\u001b[32m0\u001b[39m]) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatched_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43malignment_stream_analyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_token\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, V)\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;66;03m# Apply repetition penalty\u001b[39;00m\n\u001b[32m    510\u001b[39m ids_for_proc = generated_ids[:\u001b[32m1\u001b[39m, ...]   \u001b[38;5;66;03m# batch = 1\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ttssocketserver/chatterbox_infer/models/t3/inference/alignment_stream_analyzer.py:96\u001b[39m, in \u001b[36mAlignmentStreamAnalyzer.step\u001b[39m\u001b[34m(self, logits, next_token)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03mEmits an AlignmentAnalysisResult into the output queue, and potentially modifies the logits to force an EOS.\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# extract approximate alignment matrix chunk (1 frame at a time after the first chunk)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m aligned_attn = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_aligned_attns\u001b[49m\u001b[43m)\u001b[49m.mean(dim=\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# (N, N)\u001b[39;00m\n\u001b[32m     97\u001b[39m i, j = \u001b[38;5;28mself\u001b[39m.text_tokens_slice\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.curr_frame_pos == \u001b[32m0\u001b[39m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# first chunk has conditioning info, text tokens, and BOS token\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "async for evt in tts_model.generate_stream(\n",
    "        \"Hello every one!\",\n",
    "        language_id='en',\n",
    "        audio_prompt_path=\"/workspace/ttssocketserver/output.wav\"\n",
    "    ):\n",
    "        if evt.get(\"type\") == \"chunk\":\n",
    "            wav: torch.Tensor = evt[\"audio\"]\n",
    "            print(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa54e62-1abb-469c-8aaa-0de9c75024d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
