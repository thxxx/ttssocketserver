{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583911-fbbb-4604-a73f-cef3df232bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"trillionlabs/Tri-1.8B-Translation\",\n",
    "    dtype=\"float16\",               # GPU라면 권장\n",
    "    tensor_parallel_size=1,        # 단일 GPU 보장\n",
    "    enforce_eager=True,            # CUDA graph 캡쳐 이슈 회피\n",
    "    gpu_memory_utilization=0.5,   # 메모리 여유 확보\n",
    "    disable_log_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b618c23d-68ef-4d74-80ba-ea2bd31c61f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m prompt_old = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mTranslate into ko\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m<ko>\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m이런 날에는 할 일이 너무 많고 할 일도 많지만, 나중에 저녁을 먹을 때쯤이면.\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mTranslate into ko\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33mThere will be a chance to reflect.<ko>\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m out = \u001b[43mllm\u001b[49m.chat([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}], sampling_params=sp)\n\u001b[32m     17\u001b[39m out[\u001b[32m0\u001b[39m].outputs[\u001b[32m0\u001b[39m].text.strip()\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "sp = SamplingParams(temperature=0.1, max_tokens=512)\n",
    "\n",
    "target = \"ko\"\n",
    "text = \"\"\"There's so much to do on a day like this, lots of things to do, but maybe later when you're having dinner.\"\"\"\n",
    "\n",
    "prompt_old = f\"\"\"\n",
    "Translate into ko\\n\n",
    "{text}<ko>\n",
    "이런 날에는 할 일이 너무 많고 할 일도 많지만, 나중에 저녁을 먹을 때쯤이면.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate into ko\\n\n",
    "There will be a chance to reflect.<ko>\n",
    "\"\"\"\n",
    "out = llm.chat([{\"role\": \"user\", \"content\": prompt}], sampling_params=sp)\n",
    "out[0].outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004363e-f612-4b01-b993-37aa3a17c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a137b4-10f7-499f-b0ba-084fb76867d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd8b09e-7a71-4095-8895-a74c88ed9d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-03 03:16:27 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-03 03:16:28 [utils.py:326] non-default args: {'model': 'ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', 'enable_prefix_caching': True, 'max_num_seqs': 512, 'disable_log_stats': True}\n",
      "INFO 09-03 03:16:35 [__init__.py:711] Resolved architecture: MistralForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-03 03:16:35 [__init__.py:1750] Using max model len 32768\n",
      "INFO 09-03 03:16:36 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:37 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:37 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', speculative_config=None, tokenizer='ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:38 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m WARNING 09-03 03:16:38 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:38 [gpu_model_runner.py:1953] Starting to load model ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8...\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:38 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:38 [compressed_tensors_w8a8_int8.py:52] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:38 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:39 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55ad2526df0466f8380efc3106109f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:41 [default_loader.py:262] Loading weights took 1.41 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:41 [gpu_model_runner.py:2007] Model loading took 7.5096 GiB and 2.411257 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:49 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/09a4ce9418/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:49 [backends.py:559] Dynamo bytecode transform time: 7.66 s\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:53 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.772 s\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:54 [monitor.py:34] torch.compile takes 7.66 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:57 [gpu_worker.py:276] Available KV cache memory: 5.31 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:57 [kv_cache_utils.py:849] GPU KV cache size: 43,488 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:16:57 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 3.53x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:06<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:17:04 [gpu_model_runner.py:2708] Graph capturing finished in 6 secs, took 0.58 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=8277)\u001b[0;0m INFO 09-03 03:17:04 [core.py:214] init engine (profile, create kv cache, warmup model) took 22.59 seconds\n",
      "INFO 09-03 03:17:04 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"ByteDance-Seed/Seed-X-PPO-7B-GPTQ-Int8\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    max_num_seqs=512,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=True, \n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26aa1425-7cf4-4207-86f8-329facfb5cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f09906457bd4c99b41b3c24c7294e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9c22a1f7d441e9bceb47358487d090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안트워프의 제 팀에는 영국에 있었던 몇몇 선수들이 있었는데, 제가 그들과 맨체스터 유나이티드에 대해 이야기를 나누었을 때, 그들의 얼굴이 바뀌는 것을 바로 볼 수 있었어요.']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.\n",
    "In my team in Antwerp there were some players who were also in England and when I talked to them about Manchester United you could directly see their face change.\n",
    "<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치 어린 시절의 꿈 같았어요.\"\"\",\n",
    "]\n",
    "\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0.1,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "results = model.generate(messages, decoding_params)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98de11b2-4c90-4f5d-a1b3-0acfffb7d1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2920445f0ea2438ca0ddf3fbafa42162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6af83fe25b4c0c9158fa2303a365f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9087235927581787\n",
      "['안트워프의 제 팀에는 영국에 있었던 몇몇 선수들이 있었고, 제가 그들과 맨체스터 유나이티드에 대해 이야기를 할 때, 그들의 얼굴이 바뀌는 것을 바로 볼 수 있었습니다.']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "In my team in Antwerp there were some players who were also in England and when I talked to them about Manchester United you could directly see their face change.\n",
    "<ko>\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "# Sampling\n",
    "decoding_params = SamplingParams(temperature=0.1,\n",
    "                                 max_tokens=512,\n",
    "                                 skip_special_tokens=True)\n",
    "\n",
    "import time\n",
    "st = time.time()\n",
    "results = model.generate(messages, decoding_params)\n",
    "print(time.time() - st)\n",
    "responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27eeba-f2b0-4041-8502-74c81b321c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd79cea9-f208-4610-a4ec-ceb25d84c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4449949264526367 \n",
      " ChatCompletion(id='chatcmpl-CBYFexjpVJTh5561UskZKIjqGItv8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869306, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "1.6492297649383545 \n",
      " ChatCompletion(id='chatcmpl-CBYFfeCosk79VvphjcGxlHekhHput', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869307, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "2.170250177383423 \n",
      " ChatCompletion(id='chatcmpl-CBYFguw01L73j0yyZHwmRsHrFi3uW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 제가 그들이 관심이 있다는 것을 들었을 때는 마치, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869308, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=62, prompt_tokens=141, total_tokens=203, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "0.6452639102935791 \n",
      " ChatCompletion(id='chatcmpl-CBYFiQqvzvHfswkUKdcmQFHjbJwcD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='네, 어린 시절의 꿈 같았어요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1756869310, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_3f58d112f7', usage=CompletionUsage(completion_tokens=11, prompt_tokens=141, total_tokens=152, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "totals = 0\n",
    "for _ in range(12):\n",
    "    st = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator specializing in [English] → [Korean] translation. Your job is to incrementally translate Korean speech as it comes in.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Translate the following English sentence into Korean:\n",
    "So I think Yeah, that stays in your memory you keep remembering how big Manchester United is and so when I heard they were interested it was like Yeah, childhood dream.<ko>\n",
    "그래서 저는 그래요, 그건 당신의 기억에 남아있어요. 당신은 맨체스터 유나이티드가 얼마나 큰지 계속 기억하고 있고, 그래서 저가 그들이 관심이 있다는 것을 들었을 때는 마치...\n",
    "\"\"\"}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        # stream=True,\n",
    "        # stream_options={\"include_usage\": True},\n",
    "    )\n",
    "    \n",
    "    # sent = ''\n",
    "    \n",
    "    # pt = 0\n",
    "    # pt_cached = 0\n",
    "    # ct = 0\n",
    "    \n",
    "    # for chunk in response:\n",
    "    #     if chunk.usage and chunk.usage is not None:\n",
    "    #         if pt == 0:\n",
    "    #             # print(time.time() - st)\n",
    "    #             pt += 1\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "    #             sent += chunk.choices[0].delta.content\n",
    "    \n",
    "    print(time.time() - st, \"\\n\", response)\n",
    "    totals += time.time() - st\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1cbdc-ce39-46a3-a28d-8a88a0361250",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"And there's a dimension of human intelligence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d2562-373b-4019-a01e-ad61f33b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return new[:i] + new[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc034e-31e5-4a3c-88bf-d25ba18649af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_pr(old, new):\n",
    "    # old, new 둘 다 소문자로 변환\n",
    "    o = old.lower()\n",
    "    n = new.lower()\n",
    "\n",
    "    # 공백과 콤마 제거\n",
    "    o_clean = re.sub(r\"[ ,.]\", \"\", o)\n",
    "    n_clean = re.sub(r\"[ ,.]\", \"\", n)\n",
    "\n",
    "    # 공통 prefix 찾기\n",
    "    i = 0\n",
    "    while i < len(o_clean) and i < len(n_clean) and o_clean[i] == n_clean[i]:\n",
    "        i += 1\n",
    "\n",
    "    # old는 공통 부분까지만, 나머지는 new에서 가져오기\n",
    "    return old[:i] + new[i:]\n",
    "\n",
    "old = 'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "new = 'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.'\n",
    "\n",
    "text_pr(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de166c-2011-47aa-aee3-0c85f7b6b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['So you stop saving in US government.',\n",
    "'So you stop saving in US government bonds.',\n",
    "'So you stop saving in US government bonds and you',\n",
    "'So you stop saving in US government bonds and you start',\n",
    "'So you stop saving in US government bonds and you start saving.',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in US government bonds and you start saving',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hard',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest way.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around.',\n",
    "'So you stop saving in U.S. government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in US government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "'So you stop saving in U S government bonds and you start saving in the hardest money around, which is Bitcoin.',\n",
    "]\n",
    "\n",
    "tt = ''\n",
    "for t in texts:\n",
    "    tt = text_pr(tt, t)\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7454c-1b1d-4e8d-bfa0-573c5a186bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ['weaf', '222', '333', '444', '555']\n",
    "\n",
    "text = \"\\n\".join([f\"<{x}>\" for x in ss])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8015-d4a2-4a9e-b5a1-ddfa24711fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'de' in {'de': 123}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624cae-cb29-42e6-aba0-266203607a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ceabc-f04c-4b21-beb6-7326693be9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stt.asr import load_asr_backend\n",
    "ASR = load_asr_backend(kind=\"nemo\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdd50f-6344-46d5-8bbe-6001e3de5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "audio, sr = librosa.load(\"./utils/individualAudio.wav\", mono=True, sr=16000)\n",
    "# audio = torch.tensor(audio).to('cuda')\n",
    "pcm_bytes = (np.clip(audio, -1.0, 1.0) * 32767.0).astype(np.int16).tobytes()\n",
    "\n",
    "st = time.time()\n",
    "ASR.transcribe_pcm(pcm_bytes, sr, 1, language=\"english\")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db786f-37f9-47f1-8d04-a9814c2c68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe5693-342d-4d99-9616-6ea8eb22d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
