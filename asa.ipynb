{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937171f-dcb7-40a2-b1a8-c6934454bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.mtl_tts import ChatterboxMultilingualTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxMultilingualTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b990dce-7e20-4093-bf59-7d0b82345a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "text = \"hm..\"\n",
    "out = 'hmhm.wav'\n",
    "wav, tokens = model.generate(text, audio_prompt_path='./output.wav', language_id='en')\n",
    "print(wav.shape[-1]/24000)\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974524-6487-4a4d-9ff4-0d879babc25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11f537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a8a5-995b-479d-8aac-8f892bc9f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import torch\n",
    "\n",
    "display(Audio(wav, rate=24000))\n",
    "bi=0\n",
    "all_audios = []\n",
    "for i in [20, 40, 60, 80, 100, 120, 140, 160]:\n",
    "    cwav, _ = model.s3gen.inference(\n",
    "        speech_tokens=tokens[bi:i],\n",
    "        ref_dict=model.conds.gen,\n",
    "    )\n",
    "    all_audios.append(cwav.detach().cpu())\n",
    "    # display(Audio(cwav.detach().cpu(), rate=24000))\n",
    "    bi = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a1c8f-40c3-4aa4-8045-7b70b1ac8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_wav = torch.concat(all_audios, dim=1)\n",
    "display(Audio(summed_wav, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b900ad6-e1a7-4205-86c3-761bc79170ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec883bfc-f945-4ec5-8ca8-c38d8263081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfc614-8a79-4b4a-a9a2-89dee073d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e238b8b-55d6-406c-be8c-d8e7bf50cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox.tts import ChatterboxTTS\n",
    "import time\n",
    "from IPython.display import Audio\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d950d20-fc13-41af-b505-b5d7a0dd45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0\n",
    "st = time.time()\n",
    "wavs = []\n",
    "async for evt in model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]\n",
    "        wavs.append(wav[:, max(0, last_length-1000):])\n",
    "        print(f\"[{(wav.shape[-1]-last_length)/24000}s] - {time.time() - st}\")\n",
    "        last_length = wav.shape[-1]\n",
    "        st = time.time()\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d6f40-0423-43ec-bfdc-8ede80c9346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "\n",
    "SR = 24000\n",
    "OVERLAP = int(0.05 * SR)  # 50ms\n",
    "\n",
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0          # 모델이 지금까지 만든 전체 wav 길이\n",
    "last_tail = None         # 직전 출력 청크의 꼬리(OVERLAP 샘플)\n",
    "st = time.time()\n",
    "\n",
    "wavs = []                # overlap-add로 섞은 최종 출력 청크들\n",
    "\n",
    "async for evt in tts_model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]                    # shape: (ch, T_total_so_far)\n",
    "        device = wav.device\n",
    "        dtype = wav.dtype\n",
    "\n",
    "        # 이번에 \"새로 추가된\" 구간만 잘라오기\n",
    "        new_total = wav.shape[-1]\n",
    "        delta = new_total - last_length\n",
    "        if delta <= 0:\n",
    "            continue  # 새로 생긴 게 없으면 스킵\n",
    "\n",
    "        new_part = wav[:, last_length:new_total]  # (ch, delta)\n",
    "\n",
    "        if last_tail is None:\n",
    "            # 첫 청크면 바로 내보냄\n",
    "            out_chunk = new_part\n",
    "        else:\n",
    "            # 겹치는 길이 L (= min(OVERLAP, 새로 생긴 길이, last_tail 길이))\n",
    "            L = min(OVERLAP, new_part.shape[-1], last_tail.shape[-1])\n",
    "            if L > 0:\n",
    "                # last_tail의 마지막 L 샘플 ↔ new_part의 앞 L 샘플을 교차페이드\n",
    "                fade_in  = torch.linspace(0, 1, L, device=device, dtype=dtype)\n",
    "                fade_out = 1.0 - fade_in\n",
    "\n",
    "                mixed = last_tail[:, -L:] * fade_out + new_part[:, :L] * fade_in\n",
    "                tail  = new_part[:, L:]  # 비겹침 뒷부분\n",
    "\n",
    "                out_chunk = torch.cat([mixed, tail], dim=-1)\n",
    "            else:\n",
    "                # 겹칠 게 없으면 그냥 이어붙임\n",
    "                out_chunk = new_part\n",
    "\n",
    "        # 다음 교차페이드를 위해 꼬리 갱신\n",
    "        # (모델 기준 전체 wav의 최신 꼬리를 쓰는 게 안전)\n",
    "        new_tail_start = max(0, new_total - OVERLAP)\n",
    "        last_tail = wav[:, new_tail_start:new_total].detach()\n",
    "\n",
    "        # 사용자 출력/저장을 위해 overlap-add 결과만 모음\n",
    "        wavs.append(out_chunk)\n",
    "\n",
    "        print(f\"[{out_chunk.shape[-1]/SR:.3f}s] - {time.time() - st:.3f}\")\n",
    "        last_length = new_total\n",
    "        st = time.time()\n",
    "\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7ddd2-1d24-4789-b64d-f9030c6cf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "display(Audio(wav.cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수\n",
    "display(Audio(torch.concat(wavs, dim=-1).cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db3e6d-cd50-4d8c-b143-08685d4d5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "model.t3.speech_pos_emb.get_fixed_embedding(213)\n",
    "time.time() - st\n",
    "\n",
    "import torch\n",
    "\n",
    "st = time.time()\n",
    "emb = torch.nn.Embedding(113, 512)\n",
    "emb(torch.tensor(12))\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b655e-a453-4e9b-936d-4527fbb0db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "torch.tensor(132).to('cuda')\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451018f-2fff-4cd7-b614-124dbd15b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn((132001))\n",
    "st = time.time()\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "dist = torch.distributions.Categorical(logits=logits)  # GPU에서 동작\n",
    "next_token = dist.sample()  # (B, 1) 형태 맞추기\n",
    "print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad1e0-7cc3-41a4-98d4-967872a0ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "\n",
    "audio, sr = librosa.load('test-1.wav')\n",
    "\n",
    "text = \"Oh\"\n",
    "st = time.time()\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape, tokens.shape, time.time() - st)\n",
    "# ta.save(\"test-1.wav\", wav, model.sr)\n",
    "display(Audio('test-1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75654aa5-03ac-4e15-a31e-86bba64bf8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7c396-b929-446a-b66f-15d92124f731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c7f32-c788-4acc-9716-8418c9ea84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.tts import ChatterboxTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe3330-1b9a-437b-8bd0-b343054a5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "audio, _ = librosa.load('/workspace/chatterbox/sesame.wav', sr=16000, mono=True)\n",
    "\n",
    "text = \"I want to go home now please let me go.\"\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape[-1]/24000)\n",
    "\n",
    "out = 'just.wav'\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cb1c0-3dfd-4453-9ac1-bec435ed4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 지금 계속 한글로 말하는걸 영어로 번역하고 있어.\n",
    "# <previous utterances>는 현재 문장 이전에 이야기하던 문장이야. 번역을 위한 맥락 파악에 사용할 수 있어.\n",
    "# <speaking english>은 번역해야하는 현재 발화야.\n",
    "\n",
    "# 말을 하는걸 script로 만든 input이기 때문에, 발음 문제로 인해서 텍스트가 잘못 들어왔을 수 있어. 그걸 감안해서 번역해줘.\n",
    "\n",
    "# 출력 english를 일반 글 문장보다는 실제로 사람이 말하는 것 같은 구어체로 적어줘. 예를 들어, 같은 단어를 두번 쓰거나 뭐 ...을 쓰거나 느낌표 이런 것들 있잖아?\n",
    "# Translate into casual spoken English. 근데 너무 심하게 하진 말고, 없는 말을 만들거나 들어온 input을 왜곡하면 안돼.\n",
    "# Do not start with word like Oh, So, Uhm, Huh, etc.\n",
    "\n",
    "# 혹시 한글 문장이 아직 종결되지 않았는데 중간에 들어온 것 같으면, 출력 영어도 종결되지 않고 뒤에 이어질 수 있는 문장으로 뱉어줘.\n",
    "# 뒤에 어떤 말이 들어올지 모르니까 만약 문장이 종결되지 않았다고 판단되면, 뒤에 어떤 문장이 들어오던 현재 출력하는 문장을 이어서 완성할 수 있게 해야해.\n",
    "# 그래서 꼭 지금 한글 input을 전부 번역할 필요는 없어.\n",
    "\n",
    "# 현재까지 번역된 문장이 있으면, 그 문장을 이어서 번역해줘. 먼저 들어온 번역 문장이 잘못 되었을 수도 있어. 그래도 알아서 잘 구어체로.\n",
    "# 추가 번역되어야하는 문장만 출력해줘.\n",
    "\n",
    "# 그리고 만약 뒤에 어떤 다른 문장이 들어오던 현재의 speaking korean을 전부 <english>에 포함시켜서 번역이 완료되었다면, 마지막에 <END>를 붙여줘.\n",
    "# 뒤에 추가로 번역이 필요하면 ...을 마지막에 붙여줘.\n",
    "\n",
    "# -- INPUT --\n",
    "# <previous utterances>{hist}\n",
    "# <speaking korean> : {current_scripted_sentence}\n",
    "# <english> : {current_translated}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b822a95-ce59-4cda-a085-647a101d46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "def translate(prevScripts:str, current_scripted_sentence:str, current_translated:str, onToken, input_language:str = 'Korean', output_language:str = 'English'):\n",
    "    hist = \"\\n\".join([f\" me:{x},\" for x in prevScripts])\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4.1-mini',  # 최신 경량 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a professional translator specializing in [{input_language}] → [{output_language}] translation.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "You are translating {input_language} speech into {output_language}.\n",
    "\n",
    "<previous utterances> are the sentences spoken before the current one. Use them for context.  \n",
    "<speaking korean> is the current spoken input that needs to be translated.  \n",
    "<{output_language}> is the translation generated so far.  \n",
    "\n",
    "The input comes from speech-to-text, so there may be transcription errors due to pronunciation. Please take this into account when translating.  \n",
    "\n",
    "Output the translation in casual spoken {output_language} — like how people actually talk, with natural pauses, repetitions, or fillers such as “...” or “!” — but don’t overdo it. Do not invent new words or distort the original meaning.  \n",
    "Do not start with words like “Oh”, “So”, “Uhm”, or “Huh”.  \n",
    "\n",
    "If the {input_language} input seems incomplete (cut off mid-sentence), output an unfinished {output_language} sentence too, so it can be naturally continued. You don’t need to force a full translation of every fragment if it isn’t complete yet.\n",
    "한글 종결 어미의 특징 : ~~요. ~~니다. ~~어.\n",
    "\n",
    "If there is already some translated {output_language}, continue from it. !Do not include the previous translation in the output never!\n",
    "The earlier translation may not be perfect — refine it naturally into spoken {output_language}. Only output the additional translated part.  \n",
    "\n",
    "If the current {input_language} input is fully translated and nothing else needs to be added, end with `<END>`.  \n",
    "If more input is likely to follow, end with `...`.  \n",
    "\n",
    "# Real-Time Translation Tips\n",
    "1. **Avoid Premature Subject Translation**\n",
    "- Korean often omits or ambiguates the subject.\n",
    "- When the subject is unclear, try to infer it from prior context.\n",
    "- If there’s even slight ambiguity, avoid explicitly translating the subject (\"I\", \"we\", \"they\", etc.) and use neutral or impersonal expressions instead.\n",
    "- Example:\n",
    "  - Korean: “마케팅비를 청구해야 한다”\n",
    "  - Preferred: “Marketing costs must also be claimed.<END>”\n",
    "  - Not: “I should claim the marketing costs.<END>”\n",
    "- Example:\n",
    "  - Korean: \"내일 집에\"\n",
    "  - Preferred: \"Tomorrow,\"\n",
    "  - Not: \"Tomorrow at home, I'll \"\n",
    "\n",
    "2. Do not include the verb in the translation if no verb is spoken.\n",
    "- Korean places verbs at the end. Don't translate prematurely if the action is unknown.\n",
    "- Example: “운동장에가서 축구를…”\n",
    "  - Preferred: \"I'll go to the sports field and...\" # Still don't know whether they will play soccer or watch. Just skip the sentence, because there will be more input to come next.\n",
    "  - Not: \"I'll go to the sports field and play soccer...\" # This is not correct, because next input can be \"축구를 봤어.\"\n",
    "\n",
    "-- INPUT --  \n",
    "<previous utterances> {hist}  \n",
    "<speaking {input_language}> : {current_scripted_sentence}  \n",
    "<{output_language}> : {current_translated}  \n",
    "\"\"\"}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        user=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        prompt_cache_key=\"k2e-translator-v1-hojinkhj6051230808\",\n",
    "        stream=True,\n",
    "        stream_options={\"include_usage\": True},\n",
    "    )\n",
    "\n",
    "    sent = ''\n",
    "    first = 0\n",
    "    st = time.time()\n",
    "\n",
    "    pt = 0\n",
    "    pt_cached = 0\n",
    "    ct = 0\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.usage and chunk.usage is not None:\n",
    "            u = chunk.usage;\n",
    "            pt += u.prompt_tokens\n",
    "            pt_cached += u.prompt_tokens_details.cached_tokens\n",
    "            ct += u.completion_tokens\n",
    "        else:\n",
    "            if chunk.choices[0].delta.content != '' and chunk.choices[0].delta.content is not None:\n",
    "                onToken(chunk.choices[0].delta.content)\n",
    "                sent += chunk.choices[0].delta.content\n",
    "\n",
    "    return {\n",
    "        \"text\": sent,\n",
    "        \"prompt_tokens\": pt,\n",
    "        \"prompt_tokens_cached\": pt_cached,\n",
    "        \"completion_tokens\": ct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc031b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm.openai import translate\n",
    "import time\n",
    "\n",
    "examples = [\n",
    "    ['\"저는 다음주 쯤에 샌프란시스코로.', '가려다가 그냥 안가려구요.'],\n",
    "    ['이번 주에 회의가 있을 수도 있어.', '밥 뭐먹지?'],\n",
    "    ['오늘 아침에 밥을', '먹진 않고 간단하게 음료수만 마셨어.'],\n",
    "    ['제가 항상 듣던 말은.', '너는 왜그렇게 말이 많냐는 거였어요.'],\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    output1 = translate([], ex[0], \"\", lambda x: None)\n",
    "    output2 = translate([], f\"{ex[0]} {ex[1]}\", output1['text'], lambda x: None)\n",
    "\n",
    "    print(output1)\n",
    "    print(output2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f120f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
