{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937171f-dcb7-40a2-b1a8-c6934454bb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khj6051/chatter/lib/python3.10/site-packages/perth/perth_net/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/khj6051/chatter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 53658.47it/s]\n",
      "/home/khj6051/chatter/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "WARNING:chatterbox_infer.models.tokenizers.tokenizer:pkuseg not available - Chinese segmentation will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded PerthNet (Implicit) at step 250,000\n"
     ]
    }
   ],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.mtl_tts import ChatterboxMultilingualTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxMultilingualTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b990dce-7e20-4093-bf59-7d0b82345a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n",
      "Sampling:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhm..\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhmhm.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m wav, tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_prompt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./output.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(wav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m24000\u001b[39m)\n\u001b[1;32m      7\u001b[0m ta\u001b[38;5;241m.\u001b[39msave(out, wav, model\u001b[38;5;241m.\u001b[39msr)\n",
      "File \u001b[0;32m~/ttssocketserver/chatterbox_infer/mtl_tts.py:280\u001b[0m, in \u001b[0;36mChatterboxMultilingualTTS.generate\u001b[0;34m(self, text, language_id, audio_prompt_path, exaggeration, cfg_weight, temperature, repetition_penalty, min_p, top_p)\u001b[0m\n\u001b[1;32m    277\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(text_tokens, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), value\u001b[38;5;241m=\u001b[39meot)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 280\u001b[0m     speech_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt3_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: use the value in config\u001b[39;49;00m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# Extract only the conditional batch.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     speech_tokens \u001b[38;5;241m=\u001b[39m speech_tokens[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/chatter/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ttssocketserver/chatterbox_infer/models/t3/t3.py:348\u001b[0m, in \u001b[0;36mT3.inference\u001b[0;34m(self, t3_cond, text_tokens, initial_speech_tokens, prepend_prompt_speech_tokens, num_return_sequences, max_new_tokens, stop_on_eos, do_sample, temperature, top_p, min_p, length_penalty, repetition_penalty, cfg_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# Pass the last generated token for repetition tracking\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     last_token \u001b[38;5;241m=\u001b[39m generated_ids[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generated_ids[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatched_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malignment_stream_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_token\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, V)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Apply repetition penalty\u001b[39;00m\n\u001b[1;32m    351\u001b[0m ids_for_proc \u001b[38;5;241m=\u001b[39m generated_ids[:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]   \u001b[38;5;66;03m# batch = 1\u001b[39;00m\n",
      "File \u001b[0;32m~/ttssocketserver/chatterbox_infer/models/t3/inference/alignment_stream_analyzer.py:96\u001b[0m, in \u001b[0;36mAlignmentStreamAnalyzer.step\u001b[0;34m(self, logits, next_token)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mEmits an AlignmentAnalysisResult into the output queue, and potentially modifies the logits to force an EOS.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# extract approximate alignment matrix chunk (1 frame at a time after the first chunk)\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m aligned_attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_aligned_attns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# (N, N)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m i, j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_tokens_slice\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_frame_pos \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# first chunk has conditioning info, text tokens, and BOS token\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "text = \"hm..\"\n",
    "out = 'hmhm.wav'\n",
    "wav, tokens = model.generate(text, audio_prompt_path='./output.wav', language_id='en')\n",
    "print(wav.shape[-1]/24000)\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974524-6487-4a4d-9ff4-0d879babc25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588a8a5-995b-479d-8aac-8f892bc9f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import torch\n",
    "\n",
    "display(Audio(wav, rate=24000))\n",
    "bi=0\n",
    "all_audios = []\n",
    "for i in [20, 40, 60, 80, 100, 120, 140, 160]:\n",
    "    cwav, _ = model.s3gen.inference(\n",
    "        speech_tokens=tokens[bi:i],\n",
    "        ref_dict=model.conds.gen,\n",
    "    )\n",
    "    all_audios.append(cwav.detach().cpu())\n",
    "    # display(Audio(cwav.detach().cpu(), rate=24000))\n",
    "    bi = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a1c8f-40c3-4aa4-8045-7b70b1ac8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_wav = torch.concat(all_audios, dim=1)\n",
    "display(Audio(summed_wav, rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b900ad6-e1a7-4205-86c3-761bc79170ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec883bfc-f945-4ec5-8ca8-c38d8263081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfc614-8a79-4b4a-a9a2-89dee073d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e238b8b-55d6-406c-be8c-d8e7bf50cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox.tts import ChatterboxTTS\n",
    "import time\n",
    "from IPython.display import Audio\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d950d20-fc13-41af-b505-b5d7a0dd45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0\n",
    "st = time.time()\n",
    "wavs = []\n",
    "async for evt in model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]\n",
    "        wavs.append(wav[:, max(0, last_length-1000):])\n",
    "        print(f\"[{(wav.shape[-1]-last_length)/24000}s] - {time.time() - st}\")\n",
    "        last_length = wav.shape[-1]\n",
    "        st = time.time()\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d6f40-0423-43ec-bfdc-8ede80c9346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "\n",
    "SR = 24000\n",
    "OVERLAP = int(0.05 * SR)  # 50ms\n",
    "\n",
    "text = \"Oh, I really love it.. y'know, it's amazing.\"\n",
    "\n",
    "last_length = 0          # 모델이 지금까지 만든 전체 wav 길이\n",
    "last_tail = None         # 직전 출력 청크의 꼬리(OVERLAP 샘플)\n",
    "st = time.time()\n",
    "\n",
    "wavs = []                # overlap-add로 섞은 최종 출력 청크들\n",
    "\n",
    "async for evt in tts_model.generate_stream(text, audio_prompt_path='test-1.wav'):\n",
    "    if evt[\"type\"] == \"chunk\":\n",
    "        wav = evt[\"audio\"]                    # shape: (ch, T_total_so_far)\n",
    "        device = wav.device\n",
    "        dtype = wav.dtype\n",
    "\n",
    "        # 이번에 \"새로 추가된\" 구간만 잘라오기\n",
    "        new_total = wav.shape[-1]\n",
    "        delta = new_total - last_length\n",
    "        if delta <= 0:\n",
    "            continue  # 새로 생긴 게 없으면 스킵\n",
    "\n",
    "        new_part = wav[:, last_length:new_total]  # (ch, delta)\n",
    "\n",
    "        if last_tail is None:\n",
    "            # 첫 청크면 바로 내보냄\n",
    "            out_chunk = new_part\n",
    "        else:\n",
    "            # 겹치는 길이 L (= min(OVERLAP, 새로 생긴 길이, last_tail 길이))\n",
    "            L = min(OVERLAP, new_part.shape[-1], last_tail.shape[-1])\n",
    "            if L > 0:\n",
    "                # last_tail의 마지막 L 샘플 ↔ new_part의 앞 L 샘플을 교차페이드\n",
    "                fade_in  = torch.linspace(0, 1, L, device=device, dtype=dtype)\n",
    "                fade_out = 1.0 - fade_in\n",
    "\n",
    "                mixed = last_tail[:, -L:] * fade_out + new_part[:, :L] * fade_in\n",
    "                tail  = new_part[:, L:]  # 비겹침 뒷부분\n",
    "\n",
    "                out_chunk = torch.cat([mixed, tail], dim=-1)\n",
    "            else:\n",
    "                # 겹칠 게 없으면 그냥 이어붙임\n",
    "                out_chunk = new_part\n",
    "\n",
    "        # 다음 교차페이드를 위해 꼬리 갱신\n",
    "        # (모델 기준 전체 wav의 최신 꼬리를 쓰는 게 안전)\n",
    "        new_tail_start = max(0, new_total - OVERLAP)\n",
    "        last_tail = wav[:, new_tail_start:new_total].detach()\n",
    "\n",
    "        # 사용자 출력/저장을 위해 overlap-add 결과만 모음\n",
    "        wavs.append(out_chunk)\n",
    "\n",
    "        print(f\"[{out_chunk.shape[-1]/SR:.3f}s] - {time.time() - st:.3f}\")\n",
    "        last_length = new_total\n",
    "        st = time.time()\n",
    "\n",
    "    elif evt[\"type\"] == \"eos\":\n",
    "        print(\"✅ 스트리밍 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7ddd2-1d24-4789-b64d-f9030c6cf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "display(Audio(wav.cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수\n",
    "display(Audio(torch.concat(wavs, dim=-1).cpu().numpy(), rate=24000))   # 예: 오디오 출력 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db3e6d-cd50-4d8c-b143-08685d4d5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "model.t3.speech_pos_emb.get_fixed_embedding(213)\n",
    "time.time() - st\n",
    "\n",
    "import torch\n",
    "\n",
    "st = time.time()\n",
    "emb = torch.nn.Embedding(113, 512)\n",
    "emb(torch.tensor(12))\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b655e-a453-4e9b-936d-4527fbb0db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "torch.tensor(132).to('cuda')\n",
    "time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451018f-2fff-4cd7-b614-124dbd15b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn((132001))\n",
    "st = time.time()\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)\n",
    "print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "dist = torch.distributions.Categorical(logits=logits)  # GPU에서 동작\n",
    "next_token = dist.sample()  # (B, 1) 형태 맞추기\n",
    "print(time.time() - st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad1e0-7cc3-41a4-98d4-967872a0ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "\n",
    "audio, sr = librosa.load('test-1.wav')\n",
    "\n",
    "text = \"Oh\"\n",
    "st = time.time()\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape, tokens.shape, time.time() - st)\n",
    "# ta.save(\"test-1.wav\", wav, model.sr)\n",
    "display(Audio('test-1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75654aa5-03ac-4e15-a31e-86bba64bf8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b7c396-b929-446a-b66f-15d92124f731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c7f32-c788-4acc-9716-8418c9ea84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "from chatterbox_infer.tts import ChatterboxTTS\n",
    "import time\n",
    "\n",
    "model = ChatterboxTTS.from_pretrained(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe3330-1b9a-437b-8bd0-b343054a5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "audio, _ = librosa.load('/workspace/chatterbox/sesame.wav', sr=16000, mono=True)\n",
    "\n",
    "text = \"I want to go home now please let me go.\"\n",
    "wav, tokens = model.generate(text, audio_prompt_path=audio)\n",
    "print(wav.shape[-1]/24000)\n",
    "\n",
    "out = 'just.wav'\n",
    "ta.save(out, wav, model.sr)\n",
    "display(Audio(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cb1c0-3dfd-4453-9ac1-bec435ed4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b822a95-ce59-4cda-a085-647a101d46b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
